<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MPITutorials | Linuzb&#39;s blog</title>
  <meta name="keywords" content=" HPC ">
  <meta name="description" content="MPITutorials | Linuzb&#39;s blog">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="ref:Message Passing Interface (MPI) 摘要消息传递接口标准（MPI）是基于MPI论坛共识的消息传递库标准，MPI论坛有40多个参与组织，其中包括供应商，研究人员，软件库开发人员和用户。消息传递接口的目标是为消息传递建立一种可移植，高效且灵活的标准，该标准将广泛用于编写消息传递程序。因此，MPI是第一个标准化的，独立于供应商的消息传递库。使用MPI开发消息传递软件的">
<meta property="og:type" content="article">
<meta property="og:title" content="MPITutorials">
<meta property="og:url" content="https://levizebulon.github.io/2020/05/26/MPITutorials/index.html">
<meta property="og:site_name" content="Linuzb&#39;s blog">
<meta property="og:description" content="ref:Message Passing Interface (MPI) 摘要消息传递接口标准（MPI）是基于MPI论坛共识的消息传递库标准，MPI论坛有40多个参与组织，其中包括供应商，研究人员，软件库开发人员和用户。消息传递接口的目标是为消息传递建立一种可移植，高效且灵活的标准，该标准将广泛用于编写消息传递程序。因此，MPI是第一个标准化的，独立于供应商的消息传递库。使用MPI开发消息传递软件的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/MPITutorials/MPITutorials/prog_structure.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/MPITutorials/MPITutorials/comm_world.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="og:image" content="https://levizebulon.github.io/2020/05/26/images/page01.gif">
<meta property="article:published_time" content="2020-05-26T09:42:38.000Z">
<meta property="article:modified_time" content="2020-06-08T07:45:33.637Z">
<meta property="article:author" content="Linuzb">
<meta property="article:tag" content="HPC">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://levizebulon.github.io/2020/05/26/MPITutorials/MPITutorials/prog_structure.gif">


<link rel="icon" href="/img/avatar.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1" ></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.0.1" ></script>

<meta name="generator" content="Hexo 4.2.0"></head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="false">
  <input class="theme_blog_path" value="">
</div>

<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar.jpg" />
</a>
<div class="author">
    <span>Linuzb</span>
</div>

<div class="icon">
    
        
        <a title="rss" href="/atom.xml" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-rss"></use>
                </svg>
            
        </a>
        
    
        
        <a title="github" href="https://github.com/yelog" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-github"></use>
                </svg>
            
        </a>
        
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(34)</small></div></li>
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    
    
    </div>
    <div><a class="about  hasFriend  site_url"  href="/about">关于</a><a style="width: 50%"  class="friends">友链</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="34">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="http://yelog.org/">叶落阁</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode === 13){return false;}">
        <input id="local-search-input" class="search" type="text" placeholder="Search..." />
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none" />
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a class="color2">Machine Learning</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">工具</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">阅读</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">Linux</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">paper</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">Distribute System</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">Web</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">software</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">Others</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Algarithm</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">问题</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">论文</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">杂项</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">course</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">tools</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">distribute system</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">os</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">HPC</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">programming</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">Deep Learning</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">Computer Graphics</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <nav id="title-list-nav">
        
        <a  class=""
           href="/2020/03/28/hello-world/"
           data-tag="杂项"
           data-author="" >
            <span class="post-title" title="纪念失去的博客">纪念失去的博客</span>
            <span class="post-date" title="2020-03-28 19:08:07">2020/03/28</span>
        </a>
        
        <a  class=""
           href="/2019/01/09/Anaconda/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Anaconda">Anaconda</span>
            <span class="post-date" title="2019-01-09 09:55:46">2019/01/09</span>
        </a>
        
        <a  class=""
           href="/2019/03/20/Docker%E5%AE%89%E8%A3%85tensorflow-gpu/"
           data-tag="Machine Learning"
           data-author="" >
            <span class="post-title" title="Docker安装tensorflow-gpu">Docker安装tensorflow-gpu</span>
            <span class="post-date" title="2019-03-20 16:36:12">2019/03/20</span>
        </a>
        
        <a  class=""
           href="/2018/12/29/K2pdfopt%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/"
           data-tag="工具,阅读"
           data-author="" >
            <span class="post-title" title="K2pdfopt简明教程">K2pdfopt简明教程</span>
            <span class="post-date" title="2018-12-29 20:20:14">2018/12/29</span>
        </a>
        
        <a  class=""
           href="/2020/03/28/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%85%A8%E6%8B%BC/"
           data-tag="Linux"
           data-author="" >
            <span class="post-title" title="Linux常用命令全拼">Linux常用命令全拼</span>
            <span class="post-date" title="2020-03-28 17:33:07">2020/03/28</span>
        </a>
        
        <a  class=""
           href="/2020/03/28/MapReduce/"
           data-tag="paper,Distribute System"
           data-author="" >
            <span class="post-title" title="MapReduce">MapReduce</span>
            <span class="post-date" title="2020-03-28 17:37:00">2020/03/28</span>
        </a>
        
        <a  class=""
           href="/2019/01/02/Ubuntu%E7%BE%8E%E5%8C%96/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Ubuntu美化">Ubuntu美化</span>
            <span class="post-date" title="2019-01-02 07:32:54">2019/01/02</span>
        </a>
        
        <a  class=""
           href="/2020/03/28/VPS%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BAHexo%E5%8D%9A%E5%AE%A2/"
           data-tag="Web"
           data-author="" >
            <span class="post-title" title="VPS搭建个人Hexo博客">VPS搭建个人Hexo博客</span>
            <span class="post-date" title="2020-03-28 17:25:22">2020/03/28</span>
        </a>
        
        <a  class=""
           href="/2018/12/30/aria2%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/"
           data-tag="工具"
           data-author="" >
            <span class="post-title" title="aria2简明教程">aria2简明教程</span>
            <span class="post-date" title="2018-12-30 10:44:49">2018/12/30</span>
        </a>
        
        <a  class=""
           href="/2018/12/30/autosub%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/"
           data-tag="工具"
           data-author="" >
            <span class="post-title" title="autosub语音识别字幕生成工具">autosub语音识别字幕生成工具</span>
            <span class="post-date" title="2018-12-30 20:41:40">2018/12/30</span>
        </a>
        
        <a  class=""
           href="/2019/01/11/download-coursera-and-youtube/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="download_coursera_and_youtube">download_coursera_and_youtube</span>
            <span class="post-date" title="2019-01-11 13:30:38">2019/01/11</span>
        </a>
        
        <a  class=""
           href="/2019/03/01/shadowsocks%E2%80%93libev%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%9A%84%E9%83%A8%E7%BD%B2/"
           data-tag="software"
           data-author="" >
            <span class="post-title" title="Ubuntu shadowsocks–libev服务端的部署">Ubuntu shadowsocks–libev服务端的部署</span>
            <span class="post-date" title="2019-03-01 09:23:47">2019/03/01</span>
        </a>
        
        <a  class=""
           href="/2019/01/03/ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85TexLive/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="ubuntu下安装TexLive">ubuntu下安装TexLive</span>
            <span class="post-date" title="2019-01-03 16:56:38">2019/01/03</span>
        </a>
        
        <a  class=""
           href="/2019/04/21/ubuntu%E5%AE%89%E8%A3%85windows%E5%AD%97%E4%BD%93/"
           data-tag="Others"
           data-author="" >
            <span class="post-title" title="ubuntu安装windows字体">ubuntu安装windows字体</span>
            <span class="post-date" title="2019-04-21 23:16:24">2019/04/21</span>
        </a>
        
        <a  class=""
           href="/2019/01/04/virtulenv/"
           data-tag="software"
           data-author="" >
            <span class="post-title" title="virtualenv">virtualenv</span>
            <span class="post-date" title="2019-01-04 13:21:39">2019/01/04</span>
        </a>
        
        <a  class=""
           href="/2019/03/01/%E4%BD%BF%E7%94%A8HAProxy%E5%8A%A0%E9%80%9FShadowsocks/"
           data-tag="software"
           data-author="" >
            <span class="post-title" title="使用HAProxy加速Shadowsocks">使用HAProxy加速Shadowsocks</span>
            <span class="post-date" title="2019-03-01 09:52:51">2019/03/01</span>
        </a>
        
        <a  class=""
           href="/2019/01/08/%E6%9F%A5%E7%9C%8BCUDA%E5%92%8Ccudnn%E7%89%88%E6%9C%AC/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="查看CUDA和cudnn版本">查看CUDA和cudnn版本</span>
            <span class="post-date" title="2019-01-08 14:01:17">2019/01/08</span>
        </a>
        
        <a  class=""
           href="/2018/12/25/%E8%80%83%E7%A0%94%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%E9%A2%98/"
           data-tag="Algarithm"
           data-author="" >
            <span class="post-title" title="考研数据结构算法题">考研数据结构算法题</span>
            <span class="post-date" title="2018-12-25 16:30:19">2018/12/25</span>
        </a>
        
        <a  class=""
           href="/2019/01/01/%E8%A7%A3%E5%86%B3ubuntu1604%E4%B8%8Bwps%E4%B8%8D%E8%83%BD%E4%BD%BF%E7%94%A8%E6%90%9C%E7%8B%97%E8%BE%93%E5%85%A5%E6%B3%95%E9%97%AE%E9%A2%98/"
           data-tag="问题"
           data-author="" >
            <span class="post-title" title="解决ubuntu1604下wps不能使用搜狗输入法问题">解决ubuntu1604下wps不能使用搜狗输入法问题</span>
            <span class="post-date" title="2019-01-01 16:42:28">2019/01/01</span>
        </a>
        
        <a  class=""
           href="/2018/12/30/%E8%AE%BA%E6%96%87%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7Zotero/"
           data-tag="论文"
           data-author="" >
            <span class="post-title" title="论文管理工具Zotero">论文管理工具Zotero</span>
            <span class="post-date" title="2018-12-30 11:49:36">2018/12/30</span>
        </a>
        
        <a  class=""
           href="/2020/03/29/Linux%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91/"
           data-tag="software"
           data-author="" >
            <span class="post-title" title="Linux图像视频编辑">Linux图像视频编辑</span>
            <span class="post-date" title="2020-03-29 11:21:37">2020/03/29</span>
        </a>
        
        <a  class=""
           href="/2020/04/03/shell/"
           data-tag="course,tools"
           data-author="" >
            <span class="post-title" title="Course overview + the shell">Course overview + the shell</span>
            <span class="post-date" title="2020-04-03 17:05:16">2020/04/03</span>
        </a>
        
        <a  class=""
           href="/2020/04/07/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F6-824%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"
           data-tag="paper,distribute system"
           data-author="" >
            <span class="post-title" title="分布式系统6.824论文阅读">分布式系统6.824论文阅读</span>
            <span class="post-date" title="2020-04-07 21:03:52">2020/04/07</span>
        </a>
        
        <a  class=""
           href="/2020/04/17/%E8%A7%A3%E5%86%B3archlinux%E4%B8%8Bgnome-terminal%E9%80%8F%E6%98%8E%E9%97%AE%E9%A2%98/"
           data-tag="software"
           data-author="" >
            <span class="post-title" title="解决archlinux下gnome-terminal透明问题">解决archlinux下gnome-terminal透明问题</span>
            <span class="post-date" title="2020-04-17 09:34:37">2020/04/17</span>
        </a>
        
        <a  class=""
           href="/2020/04/18/docker-spark-jupyternotebook/"
           data-tag="software"
           data-author="" >
            <span class="post-title" title="docker+spark+jupyternotebook">docker+spark+jupyternotebook</span>
            <span class="post-date" title="2020-04-18 10:30:55">2020/04/18</span>
        </a>
        
        <a  class=""
           href="/2020/04/18/Tmux%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"
           data-tag="software"
           data-author="" >
            <span class="post-title" title="Tmux使用教程">Tmux使用教程</span>
            <span class="post-date" title="2020-04-18 11:19:44">2020/04/18</span>
        </a>
        
        <a  class=""
           href="/2020/05/06/v2ray-WS-TLS/"
           data-tag="software"
           data-author="" >
            <span class="post-title" title="v2ray+WS+TLS">v2ray+WS+TLS</span>
            <span class="post-date" title="2020-05-06 16:41:48">2020/05/06</span>
        </a>
        
        <a  class=""
           href="/2020/05/09/%E7%BD%91%E7%BB%9C%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/"
           data-tag="software"
           data-author="" >
            <span class="post-title" title="网络排查工具">网络排查工具</span>
            <span class="post-date" title="2020-05-09 08:49:49">2020/05/09</span>
        </a>
        
        <a  class=""
           href="/2020/05/13/Ubuntu%E5%BC%80%E5%90%AFbbr/"
           data-tag="os"
           data-author="" >
            <span class="post-title" title="Ubuntu开启bbr">Ubuntu开启bbr</span>
            <span class="post-date" title="2020-05-13 08:21:42">2020/05/13</span>
        </a>
        
        <a  class=""
           href="/2020/05/22/archlinux-NVIDIA-Bumblebee-CUDA-cudnn/"
           data-tag="software"
           data-author="" >
            <span class="post-title" title="archlinux+NVIDIA+Bumblebee+CUDA+cudnn">archlinux+NVIDIA+Bumblebee+CUDA+cudnn</span>
            <span class="post-date" title="2020-05-22 22:04:13">2020/05/22</span>
        </a>
        
        <a  class=""
           href="/2020/05/26/MPITutorials/"
           data-tag="HPC"
           data-author="" >
            <span class="post-title" title="MPITutorials">MPITutorials</span>
            <span class="post-date" title="2020-05-26 17:42:38">2020/05/26</span>
        </a>
        
        <a  class=""
           href="/2020/05/28/SparkOverview/"
           data-tag="software,distribute system"
           data-author="" >
            <span class="post-title" title="SparkOverview">SparkOverview</span>
            <span class="post-date" title="2020-05-28 10:12:42">2020/05/28</span>
        </a>
        
        <a  class=""
           href="/2020/05/28/RDD%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"
           data-tag="distribute system,programming"
           data-author="" >
            <span class="post-title" title="RDD编程指南">RDD编程指南</span>
            <span class="post-date" title="2020-05-28 11:15:51">2020/05/28</span>
        </a>
        
        <a  class=""
           href="/2020/06/13/meshCNN/"
           data-tag="Deep Learning,Computer Graphics"
           data-author="" >
            <span class="post-title" title="meshCNN">meshCNN</span>
            <span class="post-date" title="2020-06-13 21:06:46">2020/06/13</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-MPITutorials" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">MPITutorials</h1>
    
    <div class="article-meta">
        
        
        
        
        <span class="tag">
            
            <a class="color4">HPC</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title='更新时间: 2020-06-08 15:45:33'>2020-05-26 17:42</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#摘要"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是MPI"><span class="toc-text">什么是MPI</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#一种接口规范"><span class="toc-text">一种接口规范</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编程模型"><span class="toc-text">编程模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#文档"><span class="toc-text">文档</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLNL-MPI-Implementations-and-Compilers"><span class="toc-text">LLNL MPI Implementations and Compilers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#开始学习"><span class="toc-text">开始学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#一般的MPI编程结构"><span class="toc-text">一般的MPI编程结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#头文件"><span class="toc-text">头文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Communicators-and-Groups"><span class="toc-text">Communicators and Groups:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Rank"><span class="toc-text">Rank:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Error-Handling"><span class="toc-text">Error Handling:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#环境管理程序"><span class="toc-text">环境管理程序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Init"><span class="toc-text">MPI_Init</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Comm-size"><span class="toc-text">MPI_Comm_size</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Comm-rank"><span class="toc-text">MPI_Comm_rank</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Abort"><span class="toc-text">MPI_Abort</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Get-processor-name"><span class="toc-text">MPI_Get_processor_name</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Get-version"><span class="toc-text">MPI_Get_version</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Initialized"><span class="toc-text">MPI_Initialized</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Wtime"><span class="toc-text">MPI_Wtime</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Wtick"><span class="toc-text">MPI_Wtick</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Finalize"><span class="toc-text">MPI_Finalize</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Examples-Environment-Management-Routines"><span class="toc-text">Examples: Environment Management Routines</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MPI-Exercise-1"><span class="toc-text">MPI Exercise 1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#点对点通讯程序"><span class="toc-text">点对点通讯程序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#一般概念"><span class="toc-text">一般概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#First-a-Simple-Example"><span class="toc-text">First, a Simple Example:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#点对点操作的类型"><span class="toc-text">点对点操作的类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Buffering"><span class="toc-text">Buffering:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#阻塞和非阻塞"><span class="toc-text">阻塞和非阻塞:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#顺序和公平"><span class="toc-text">顺序和公平</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI消息传递例程参数"><span class="toc-text">MPI消息传递例程参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Buffer"><span class="toc-text">Buffer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Count"><span class="toc-text">Data Count</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Type"><span class="toc-text">Data Type</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Destination"><span class="toc-text">Destination</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Source"><span class="toc-text">Source</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tag"><span class="toc-text">Tag</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Communicator"><span class="toc-text">Communicator</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Status"><span class="toc-text">Status</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Request"><span class="toc-text">Request</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Blocking-Message-Passing-Routines"><span class="toc-text">Blocking Message Passing Routines</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Send"><span class="toc-text">MPI_Send</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Recv"><span class="toc-text">MPI_Recv</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Ssend"><span class="toc-text">MPI_Ssend </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Sendrecv"><span class="toc-text">MPI_Sendrecv</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Wait"><span class="toc-text">MPI_Wait</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Waitany"><span class="toc-text">MPI_Waitany</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Waitall"><span class="toc-text">MPI_Waitall</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Waitsome"><span class="toc-text">MPI_Waitsome</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Probe"><span class="toc-text">MPI_Probe</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Get-count"><span class="toc-text">MPI_Get_count </span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Examples-Blocking-Message-Passing-Routines"><span class="toc-text">Examples: Blocking Message Passing Routines</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#非阻塞消息传递例程"><span class="toc-text">非阻塞消息传递例程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Isend"><span class="toc-text">MPI_Isend</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Irecv"><span class="toc-text">MPI_Irecv</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Issend"><span class="toc-text">MPI_Issend</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Test"><span class="toc-text">MPI_Test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Testany"><span class="toc-text">MPI_Testany</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Testall"><span class="toc-text">MPI_Testall</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Testsome"><span class="toc-text">MPI_Testsome</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Iprobe"><span class="toc-text">MPI_Iprobe</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例-非阻塞消息传递例程"><span class="toc-text">示例:非阻塞消息传递例程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#集体通信例程"><span class="toc-text">集体通信例程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#集体作业的类型"><span class="toc-text">集体作业的类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Scope"><span class="toc-text">Scope:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#编程注意事项和限制"><span class="toc-text">编程注意事项和限制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#集合通信例程"><span class="toc-text">集合通信例程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Barrier"><span class="toc-text">MPI_Barrier</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Bcast"><span class="toc-text">MPI_Bcast</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Scatter"><span class="toc-text">MPI_Scatter</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Gather"><span class="toc-text">MPI_Gather</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Allgather"><span class="toc-text">MPI_Allgather</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Reduce"><span class="toc-text"> MPI_Reduce </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Allreduce"><span class="toc-text"> MPI_Allreduce </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Reduce-scatter"><span class="toc-text"> MPI_Reduce_scatter </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Alltoall"><span class="toc-text"> MPI_Alltoall </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Scan"><span class="toc-text"> MPI_Scan </span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#例子-集体通信"><span class="toc-text">例子:集体通信</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#派生数据类型"><span class="toc-text">派生数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#派生数据类型例程"><span class="toc-text">派生数据类型例程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Type-contiguous"><span class="toc-text"> MPI_Type_contiguous</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Type-vector"><span class="toc-text">MPI_Type_vector</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Type-hvector"><span class="toc-text">MPI_Type_hvector</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Type-indexed"><span class="toc-text">MPI_Type_indexed</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Type-hindexed"><span class="toc-text"> MPI_Type_hindexed </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Type-struct"><span class="toc-text">MPI_Type_struct</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Type-extent"><span class="toc-text">MPI_Type_extent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Type-commit"><span class="toc-text">MPI_Type_commit</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MPI-Type-free"><span class="toc-text">MPI_Type_free</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例-连续派生数据类型"><span class="toc-text">示例:连续派生数据类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例-向量派生的数据类型"><span class="toc-text">示例:向量派生的数据类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例-索引派生数据类型"><span class="toc-text">示例:索引派生数据类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例-结构派生的数据类型"><span class="toc-text">示例:结构派生的数据类型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#组和通信子管理例程"><span class="toc-text">组和通信子管理例程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Groups-vs-Communicators"><span class="toc-text">Groups vs. Communicators:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#组和通信器对象的主要用途"><span class="toc-text">组和通信器对象的主要用途</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编程注意事项和限制-1"><span class="toc-text">编程注意事项和限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Group-and-Communicator-Management-Routines"><span class="toc-text">Group and Communicator Management Routines</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#虚拟拓扑-Virtual-Topologies"><span class="toc-text">虚拟拓扑(Virtual Topologies)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What-Are-They"><span class="toc-text">What Are They?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-Use-Them"><span class="toc-text">Why Use Them?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#虚拟拓扑的例程"><span class="toc-text">虚拟拓扑的例程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#简要介绍一下MPI-2和MPI-3"><span class="toc-text">简要介绍一下MPI-2和MPI-3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-2"><span class="toc-text">MPI-2:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-3"><span class="toc-text">MPI-3:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#More-Information-on-MPI-2-and-MPI-3"><span class="toc-text">More Information on MPI-2 and MPI-3:</span></a></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>ref:<a href="https://computing.llnl.gov/tutorials/mpi/" target="_blank" rel="noopener">Message Passing Interface (MPI)</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>消息传递接口标准（MPI）是基于MPI论坛共识的消息传递库标准，MPI论坛有40多个参与组织，其中包括供应商，研究人员，软件库开发人员和用户。消息传递接口的目标是为消息传递建立一种可移植，高效且灵活的标准，该标准将广泛用于编写消息传递程序。因此，MPI是第一个标准化的，独立于供应商的消息传递库。使用MPI开发消息传递软件的优势与可移植性，效率和灵活性的设计目标紧密匹配。 MPI不是IEEE或ISO标准，但实际上已成为在HPC平台上编写消息传递程序的“行业标准”。</p>
<p>本教程的目的是教那些不熟悉MPI的人如何根据MPI标准开发和运行并行程序。提出的主要主题集中于对新MPI程序员最有用的主题。本教程首先介绍MPI入门，背景和基本信息。接下来是对MPI例程的详细介绍，这些例程对新MPI程序员最有用，包括MPI环境管理，点对点通信和集体通信例程。提供了C和Fortran中的大量示例以及实验室练习。</p>
<p>教程材料还包括更高级的主题，例如“派生数据类型”，“组和Communicator管理例程”以及“虚拟拓扑”。但是，这些实际上并没有在讲座中介绍，而是为那些有兴趣的人提供“进一步的阅读”。</p>
<h2 id="什么是MPI"><a href="#什么是MPI" class="headerlink" title="什么是MPI"></a>什么是MPI</h2><h3 id="一种接口规范"><a href="#一种接口规范" class="headerlink" title="一种接口规范"></a>一种接口规范</h3><ul>
<li>M P I = Message Passing Interface</li>
<li>MPI是针对消息传递库的开发人员和用户的规范。 就其本身而言，它不是一个库-而是有关该库应该是什么的规范。</li>
<li>简而言之，消息传递接口的目的是为编写消息传递程序提供广泛使用的标准。 该接口尝试是：<ul>
<li>Practical </li>
<li>Portable </li>
<li>Efficient </li>
<li>Flexible </li>
</ul>
</li>
<li>MPI标准已进行了许多修订，最新版本为MPI-3.x。</li>
<li>已经为C和Fortran90语言绑定定义了接口规范：</li>
<li>实际的MPI库实现在支持的MPI标准的版本和功能方面有所不同。 开发人员/用户将需要意识到这一点。</li>
</ul>
<h3 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h3><ul>
<li>MPI最初是为分布式内存体系结构设计的，该体系结构在当时（1980年代-1990年代初期）变得越来越流行。</li>
<li>随着架构趋势的变化，共享内存SMP通过网络进行组合，从而创建了混合分布式内存/共享内存系统。</li>
<li>MPI实现者调整了其库，以无缝处理两种类型的基础内存体系结构。 他们还采用/开发了处理不同互连和协议的方式。</li>
<li>如今，MPI几乎可以在任何硬件平台上运行：<ul>
<li>Distributed Memory </li>
<li>Shared Memory </li>
<li>Hybrid</li>
</ul>
</li>
<li>但是，无论机器的基础物理体系结构如何，编程模型显然仍然是分布式内存模型。</li>
<li>所有并行性都是明确的：程序员负责正确识别并行性并使用MPI构造实现并行算法。</li>
</ul>
<h3 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h3><ul>
<li>有关MPI标准所有版本的文档，请访问：<a href="http：//www.mpi-forum.org/docs/">http：//www.mpi-forum.org/docs/</a>。</li>
</ul>
<h2 id="LLNL-MPI-Implementations-and-Compilers"><a href="#LLNL-MPI-Implementations-and-Compilers" class="headerlink" title="LLNL MPI Implementations and Compilers"></a>LLNL MPI Implementations and Compilers</h2><h2 id="开始学习"><a href="#开始学习" class="headerlink" title="开始学习"></a>开始学习</h2><h3 id="一般的MPI编程结构"><a href="#一般的MPI编程结构" class="headerlink" title="一般的MPI编程结构"></a>一般的MPI编程结构</h3><p><img src="MPITutorials/prog_structure.gif" alt="prog_structure"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#include "mpi.h"</span></span><br><span class="line"><span class="comment">#include &lt;stdio.h&gt;</span></span><br><span class="line"><span class="comment">#include &lt;stdlib.h&gt;</span></span><br><span class="line"></span><br><span class="line">int main (int argc, char *argv[])</span><br><span class="line">&#123;</span><br><span class="line">int numtasks, rank, dest, <span class="built_in">source</span>, rc, count, tag=1;</span><br><span class="line">char inmsg, outmsg=<span class="string">'x'</span>;</span><br><span class="line">MPI_Status Stat;</span><br><span class="line"></span><br><span class="line">MPI_Init(&amp;argc,&amp;argv);</span><br><span class="line">MPI_Comm_size(MPI_COMM_WORLD, &amp;numtasks);</span><br><span class="line">MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (rank == 0) &#123;</span><br><span class="line">  dest = 1;</span><br><span class="line">  <span class="built_in">source</span> = 1;</span><br><span class="line">  rc = MPI_Send(&amp;outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);</span><br><span class="line">  rc = MPI_Recv(&amp;inmsg, 1, MPI_CHAR, <span class="built_in">source</span>, tag, MPI_COMM_WORLD, &amp;Stat);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (rank == 1) &#123;</span><br><span class="line">  dest = 0;</span><br><span class="line">  <span class="built_in">source</span> = 0;</span><br><span class="line">  rc = MPI_Recv(&amp;inmsg, 1, MPI_CHAR, <span class="built_in">source</span>, tag, MPI_COMM_WORLD, &amp;Stat);</span><br><span class="line">  rc = MPI_Send(&amp;outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h3><ul>
<li><p>所有进行MPI库调用的程序都必需。</p>
<table border="1">
  <tr>
      <th>C include file </th>
      <th>Fortran include file</th>
  </tr>
  <tr>
      <td>#include "mpi.h" </td>
      <td>    include 'mpif.h' </td>
  </tr>
</table>
</li>
<li><p>MPI调用格式</p>
<table>
  <tr>
      <th colspan=2>C Binding</th>
  </tr>
  <tr>
      <th>Format:</th>
      <td>rc = MPI_Xxxxx(parameter, ... )</td>
  </tr>
  <tr>
      <th>Example:</th>
      <td>rc = MPI_Bsend(&buf,count,type,dest,tag,comm) </td>
  <tr>
      <th>Error code:</th>
      <td>Returned as "rc". MPI_SUCCESS if successful </td>
  </tr>
</table>

</li>
</ul>
<h3 id="Communicators-and-Groups"><a href="#Communicators-and-Groups" class="headerlink" title="Communicators and Groups:"></a>Communicators and Groups:</h3><ul>
<li>MPI使用称为通信器和组的对象来定义哪些进程集合可以相互通信。</li>
<li>大多数MPI例程都要求您指定一个通讯器作为参数。</li>
<li>大多数MPI例程都要求您指定一个通讯器作为参数。</li>
<li>通讯器和组将在后面详细介绍。 现在，只要需要通信器，就只需使用MPI_COMM_WORLD-它是预定义的通信器，它包含所有MPI进程。<br><img src="MPITutorials/comm_world.gif" alt="comm_world"></li>
</ul>
<h3 id="Rank"><a href="#Rank" class="headerlink" title="Rank:"></a>Rank:</h3><ul>
<li>在通信器中，每个进程都有自己的唯一整数标识符，该标识符在进程初始化时由系统分配。 rank有时也称为“任务ID”。 rank是连续的，从零开始。</li>
<li>程序员用来指定消息的源和目标。 通常由应用程序有条件地用来控制程序的执行（如果rank = 0则执行此操作，如果rank = 1则执行此操作）。</li>
</ul>
<h3 id="Error-Handling"><a href="#Error-Handling" class="headerlink" title="Error Handling:"></a>Error Handling:</h3><ul>
<li>如上面“ MPI调用的格式”部分中所述，大多数MPI例程都包含返回/错误代码参数。</li>
<li>但是，根据MPI标准，如果发生错误，MPI调用的默认行为是中止。 这意味着您可能将无法捕获MPI_SUCCESS（零）以外的返回/错误代码。</li>
<li>该标准确实提供了覆盖此默认错误处理程序的方法。 此处提供有关如何执行此操作的讨论。 您也可以参考<a href="http://www.mpi-forum.org/docs/" target="_blank" rel="noopener">http://www.mpi-forum.org/docs/</a>上相关MPI标准文档的错误处理部分。</li>
<li>向用户显示的错误类型取决于实现。</li>
</ul>
<h2 id="环境管理程序"><a href="#环境管理程序" class="headerlink" title="环境管理程序"></a>环境管理程序</h2><p>这套例程用于询问和设置MPI执行环境，并涵盖了多种目的，例如初始化和终止MPI环境，查询rank的identity，查询MPI库的版本等。大多数常用的例程如下所述。</p>
<h3 id="MPI-Init"><a href="#MPI-Init" class="headerlink" title="MPI_Init"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Init.txt" target="_blank" rel="noopener">MPI_Init</a></h3><p>初始化MPI执行环境。 必须在每个MPI程序中调用此函数，必须在任何其他MPI函数之前调用此函数，并且在MPI程序中只能调用一次。 对于C程序，MPI_Init可以用于将命令行参数传递给所有进程，尽管这不是标准要求的，并且取决于实现。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Init (&amp;argc,&amp;argv)</span><br><span class="line">MPI_INIT (ierr)</span><br></pre></td></tr></table></figure>

<h3 id="MPI-Comm-size"><a href="#MPI-Comm-size" class="headerlink" title="MPI_Comm_size"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_size.txt" target="_blank" rel="noopener">MPI_Comm_size</a></h3><p>返回指定通信器中MPI进程的总数，例如MPI_COMM_WORLD。 如果通信器是MPI_COMM_WORLD，则它表示应用程序可用的MPI任务数。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Comm_size (comm,&amp;<span class="built_in">size</span>)</span><br><span class="line">MPI_COMM_SIZE (comm,<span class="built_in">size</span>,ierr)</span><br></pre></td></tr></table></figure>

<h3 id="MPI-Comm-rank"><a href="#MPI-Comm-rank" class="headerlink" title="MPI_Comm_rank"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_rank.txt" target="_blank" rel="noopener">MPI_Comm_rank</a></h3><p>返回指定通信器中调用MPI进程的rank。 最初，在通信器MPI_COMM_WORLD中，将为每个进程分配一个介于0和任务数-1之间的唯一整数rank。 该rank通常称为任务ID。 如果一个进程与其他通讯器相关联，那么在每个通讯器中也将具有唯一的rank。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Comm_rank (comm,&amp;rank)</span><br><span class="line">MPI_COMM_RANK (comm,rank,ierr)</span><br></pre></td></tr></table></figure>

<h3 id="MPI-Abort"><a href="#MPI-Abort" class="headerlink" title="MPI_Abort"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Abort.txt" target="_blank" rel="noopener">MPI_Abort</a></h3><p>终止与通信器关联的所有MPI进程。 在大多数MPI实现中，无论指定哪个通信器，它都会终止所有进程。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Abort (comm,errorcode)</span><br><span class="line">MPI_ABORT (comm,errorcode,ierr)</span><br></pre></td></tr></table></figure>

<h3 id="MPI-Get-processor-name"><a href="#MPI-Get-processor-name" class="headerlink" title="MPI_Get_processor_name"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_processor_name.txt" target="_blank" rel="noopener">MPI_Get_processor_name</a></h3><p>返回进程名称。 还返回名称的长度。 “名称”的缓冲区的大小必须至少为MPI_MAX_PROCESSOR_NAME个字符。 返回到“名称”中的是与实现相关的-可能与“主机名”或“主机” shell命令的输出不同。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Get_processor_name (&amp;name,&amp;resultlength)</span><br><span class="line">MPI_GET_PROCESSOR_NAME (name,resultlength,ierr)</span><br></pre></td></tr></table></figure>

<h3 id="MPI-Get-version"><a href="#MPI-Get-version" class="headerlink" title="MPI_Get_version"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_version.txt" target="_blank" rel="noopener">MPI_Get_version</a></h3><p>返回由库实现的MPI标准的版本和Subversion。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Get_version (&amp;version,&amp;subversion)</span><br><span class="line">MPI_GET_VERSION (version,subversion,ierr)</span><br></pre></td></tr></table></figure>

<h3 id="MPI-Initialized"><a href="#MPI-Initialized" class="headerlink" title="MPI_Initialized"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Initialized.txt" target="_blank" rel="noopener">MPI_Initialized</a></h3><p>指示是否已调用MPI_Init-将标志返回为逻辑true（1）或false（0）。 MPI要求每个进程仅一次调用MPI_Init。 对于想要使用MPI并准备在必要时调用MPI_Init的模块，这可能会带来问题。 MPI_Initialized解决了此问题。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Initialized (&amp;flag)</span><br><span class="line">MPI_INITIALIZED (flag,ierr)</span><br></pre></td></tr></table></figure>

<h3 id="MPI-Wtime"><a href="#MPI-Wtime" class="headerlink" title="MPI_Wtime"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Wtime.txt" target="_blank" rel="noopener">MPI_Wtime</a></h3><p>返回调用处理器上经过的挂钟时间，以秒为单位（双精度）。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Wtime ()</span><br><span class="line">MPI_WTIME ()</span><br></pre></td></tr></table></figure>

<h3 id="MPI-Wtick"><a href="#MPI-Wtick" class="headerlink" title="MPI_Wtick"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Wtick.txt" target="_blank" rel="noopener">MPI_Wtick</a></h3><p>返回MPI_Wtime的分辨率（以秒为单位）（双精度）。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Wtick ()</span><br><span class="line">MPI_WTICK ()</span><br></pre></td></tr></table></figure>

<h3 id="MPI-Finalize"><a href="#MPI-Finalize" class="headerlink" title="MPI_Finalize"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Finalize.txt" target="_blank" rel="noopener">MPI_Finalize</a></h3><p>终止MPI执行环境。 此函数应该是每个MPI程序中最后一个调用的MPI例程-此后不得再调用其他MPI例程。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Finalize ()</span><br><span class="line">MPI_FINALIZE (ierr)</span><br></pre></td></tr></table></figure>

<h2 id="Examples-Environment-Management-Routines"><a href="#Examples-Environment-Management-Routines" class="headerlink" title="Examples: Environment Management Routines"></a>Examples: Environment Management Routines</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// required MPI include file  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"mpi.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">   <span class="keyword">int</span>  numtasks, rank, len, rc; </span><br><span class="line">   <span class="keyword">char</span> hostname[MPI_MAX_PROCESSOR_NAME];</span><br><span class="line"></span><br><span class="line">   <span class="comment">// initialize MPI  </span></span><br><span class="line">   MPI_Init(&amp;argc,&amp;argv);</span><br><span class="line"></span><br><span class="line">   <span class="comment">// get number of tasks </span></span><br><span class="line">   MPI_Comm_size(MPI_COMM_WORLD,&amp;numtasks);</span><br><span class="line"></span><br><span class="line">   <span class="comment">// get my rank  </span></span><br><span class="line">   MPI_Comm_rank(MPI_COMM_WORLD,&amp;rank);</span><br><span class="line"></span><br><span class="line">   <span class="comment">// this one is obvious  </span></span><br><span class="line">   MPI_Get_processor_name(hostname, &amp;len);</span><br><span class="line">   <span class="built_in">printf</span> (<span class="string">"Number of tasks= %d My rank= %d Running on %s\n"</span>, numtasks,rank,hostname);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// do some work with message passing </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="comment">// done with MPI  </span></span><br><span class="line">   MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MPI-Exercise-1"><a href="#MPI-Exercise-1" class="headerlink" title="MPI Exercise 1"></a>MPI Exercise 1</h2><p><strong><a href="https://computing.llnl.gov/tutorials/mpi/exercise.html" target="_blank" rel="noopener"> GO TO THE EXERCISE HERE </a></strong></p>
<h2 id="点对点通讯程序"><a href="#点对点通讯程序" class="headerlink" title="点对点通讯程序"></a>点对点通讯程序</h2><h3 id="一般概念"><a href="#一般概念" class="headerlink" title="一般概念"></a>一般概念</h3><h4 id="First-a-Simple-Example"><a href="#First-a-Simple-Example" class="headerlink" title="First, a Simple Example:"></a>First, a Simple Example:</h4><p><strong>以后再翻译</strong></p>
<h4 id="点对点操作的类型"><a href="#点对点操作的类型" class="headerlink" title="点对点操作的类型"></a>点对点操作的类型</h4><ul>
<li>MPI点对点操作通常涉及两个(而且只有两个)不同MPI任务之间的消息传递。一个任务执行发送操作，另一个任务执行匹配的接收操作。</li>
<li>有不同类型的发送和接收例程用于不同的目的。例如<ul>
<li>同步发送</li>
<li>阻塞发送 / 阻塞接收</li>
<li>非阻塞发送和非阻塞接收</li>
<li>缓冲发送</li>
<li>联合发送/接收</li>
<li>“准备好”发送</li>
</ul>
</li>
<li>任何类型的发送例程都可以与任何类型的接收例程配对。</li>
<li>MPI还提供了几个与发送-接收操作相关的例程，比如那些用于等待消息到达或探测消息是否到达的例程。</li>
</ul>
<h4 id="Buffering"><a href="#Buffering" class="headerlink" title="Buffering:"></a>Buffering:</h4><ul>
<li>在一个完美的世界中，每个发送操作都将与它匹配的接收操作完全同步。这种情况很少发生。无论如何，MPI实现必须能够在两个任务不同步时处理存储数据。</li>
<li>考虑以下两种情况<ul>
<li>发送操作在接收准备就绪前5秒发生，在等待接收时，消息在哪里？</li>
<li>多个发送到达同一个接收任务，一次只能接收一个发送——正在“备份”的消息会发生什么</li>
</ul>
</li>
<li>MPI实现(不是MPI标准)决定在这些类型的情况下对数据进行什么处理。通常，保留一个系统缓冲区来保存传输中的数据。例如<br>此处应该有图片</li>
<li>系统缓冲区空间为：<ul>
<li>对程序员来说是不透明的，完全由MPI库管理</li>
<li>一种很容易耗尽的有限资源</li>
<li>通常是神秘的，没有很好的记录</li>
<li>能够存在于发送端、接收端或两者都存在</li>
<li>可以提高程序性能，因为它允许发送-接收操作是异步的。</li>
</ul>
</li>
<li>用户管理的地址空间(即你的程序变量)称为<strong>应用程序缓冲区</strong>。MPI还提供了一个用户管理的发送缓冲区。</li>
</ul>
<h4 id="阻塞和非阻塞"><a href="#阻塞和非阻塞" class="headerlink" title="阻塞和非阻塞:"></a>阻塞和非阻塞:</h4><ul>
<li>大多数MPI点对点例程可以在阻塞或非阻塞模式下使用。</li>
<li><strong>阻塞：</strong><ul>
<li>阻塞发送例程只会在安全修改应用程序缓冲区(您的发送数据)以便重用后“返回”。安全意味着修改不会影响接收任务所需的数据。安全并不意味着实际收到了数据——它很可能位于系统缓冲区中。</li>
<li>阻塞发送可以是同步的，这意味着与接收任务发生握手以确认安全发送。</li>
<li>阻塞发送可以是异步的，如果一个系统缓冲区被用来保存数据，最终交付到接收。</li>
<li>阻塞接收只在数据已经到达之后，并且应用程序准备使用的时候“返回”。</li>
</ul>
</li>
<li><strong>非阻塞：</strong><ul>
<li>非阻塞发送和接收例程的行为类似——它们几乎会立即返回。它们不等待任何通信事件完成，比如消息从用户内存复制到系统缓冲区空间，或者消息的实际到达。</li>
<li>非阻塞操作只是“请求”MPI库在能够执行操作时执行该操作。用户无法预测何时会发生这种情况。</li>
<li>除非您知道请求的非阻塞操作实际上是由库执行的，否则修改应用程序缓冲区(变量空间)是不安全的。有一些“等待”的例程用来做这个。</li>
<li>非阻塞通信主要用于通信重叠计算和利用可能的性能增益。</li>
</ul>
</li>
</ul>
<table cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top">
<th>Blocking Send</th>
<th>Non-blocking Send</th>
</tr><tr valign="top">
<td width="50%"><pre>myvar = 0;

<p>for (i=1; i&lt;ntasks; i++) {<br>   task = i;<br>   <font color="#DF4442">MPI_Send (&amp;myvar … … task …);<br></font>   myvar = myvar + 2</p>
<p>   /* do some work */</p>
<p>   }</p>
<p></pre></td><td width="50%"><pre>myvar = 0;</p>
<p>for (i=1; i&lt;ntasks; i++) {<br>   task = i;<br>   <font color="#DF4442">MPI_Isend (&amp;myvar ... ... task ...);</font><br>   myvar = myvar + 2;</p>
<p>   /* do some work */</p>
<p>   <font color="#DF4442">MPI_Wait (...);</font><br>   }</p>
<p></pre></td></p>
</tr><tr valign="top">
<td align="center"><b>Safe.  Why?<b></b></b></td>
<td align="center"><b>Unsafe. Why?<b></b></b></td>
</tr></tbody></table>

<h4 id="顺序和公平"><a href="#顺序和公平" class="headerlink" title="顺序和公平"></a>顺序和公平</h4><ul>
<li><strong>顺序:</strong><ul>
<li>MPI保证消息不会相互超越。</li>
<li>如果发送方向同一目的地连续发送两条消息(消息1和消息2)，并且两者匹配相同的receive，则receive操作将在消息2之前接收消息1。</li>
<li>如果接收者连续发送了两个Receive (Receive 1和Receive 2)，并且两者都在寻找相同的消息，则Receive 1将在Receive 2之前接收该消息。</li>
<li>如果有多个线程参与通信操作，则顺序规则不适用。</li>
</ul>
</li>
<li><strong>公平：</strong><ul>
<li>MPI不能保证公平-它取决于程序员来防止“饥饿操作”。</li>
<li>示例:任务0向任务2发送一条消息。但是，task 1发送一个与task 2接收到的消息相匹配的竞争消息。只有一个发送将完成。</li>
</ul>
</li>
</ul>
<h3 id="MPI消息传递例程参数"><a href="#MPI消息传递例程参数" class="headerlink" title="MPI消息传递例程参数"></a>MPI消息传递例程参数</h3><p>MPI点对点通信例程通常具有采用以下格式之一的参数列表：</p>
<table width="90%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr><td bgcolor="#FOF5FE"><b>Blocking sends
    </b></td><td><tt><b><nobr>
    MPI_Send(buffer,count,type,dest,tag,comm) 
</nobr></b></tt></td></tr><tr><td bgcolor="#FOF5FE"><b>Non-blocking sends
    </b></td><td><tt><b><nobr>
    MPI_Isend(buffer,count,type,dest,tag,comm,request) 
</nobr></b></tt></td></tr><tr><td bgcolor="#FOF5FE"><b>Blocking receive
    </b></td><td><tt><b><nobr>
    MPI_Recv(buffer,count,type,source,tag,comm,status) 
</nobr></b></tt></td></tr><tr><td bgcolor="#FOF5FE"><b>Non-blocking receive
    </b></td><td><tt><b><nobr>
    MPI_Irecv(buffer,count,type,source,tag,comm,request) </nobr></b></tt></td>
</tr></tbody></table>

<h3 id="Buffer"><a href="#Buffer" class="headerlink" title="Buffer"></a>Buffer</h3><p>引用要发送或接收的数据的程序（应用程序）地址空间。 在大多数情况下，这只是发送/接收的变量名。 对于C程序，此参数通过引用传递，并且通常必须在前面加上一个＆符：＆var1</p>
<h3 id="Data-Count"><a href="#Data-Count" class="headerlink" title="Data Count"></a>Data Count</h3><p>指示要发送的特定类型的数据元素的数量。</p>
<h3 id="Data-Type"><a href="#Data-Type" class="headerlink" title="Data Type"></a>Data Type</h3><p>出于可移植性的考虑，MPI预定义了其基本数据类型。 下表列出了标准要求的那些内容。</p>
<table width="90%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr><th colspan="2">C Data Types</th>
    <th colspan="2">Fortran Data Types</th>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_CHAR</b></tt></td>
    <td>char</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_CHARACTER</b></tt></td>
    <td>character(1)</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_WCHAR</b></tt></td>
    <td>wchar_t - wide character</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_SHORT</b></tt></td>
    <td>signed short int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_INT</b></tt></td>
    <td>signed int</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_INTEGER<br><font color="gray">MPI_INTEGER1
    <br>MPI_INTEGER2<br>MPI_INTEGER4</font></b></tt></td>
    <td>integer<br>integer*1<br>integer*2<br>integer*4</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_LONG</b></tt></td>
    <td>signed long int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_LONG_LONG_INT
    <br>MPI_LONG_LONG</b></tt></td>
    <td>signed long long int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_SIGNED_CHAR</b></tt></td>
    <td>signed char</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UNSIGNED_CHAR</b></tt></td>
    <td>unsigned char</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UNSIGNED_SHORT</b></tt></td>
    <td>unsigned short int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UNSIGNED</b></tt></td>
    <td>unsigned int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UNSIGNED_LONG</b></tt></td>
    <td>unsigned long int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UNSIGNED_LONG_LONG</b></tt></td>
    <td>unsigned long long int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_FLOAT</b></tt></td>
    <td>float</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_REAL<br><font color="gray">MPI_REAL2
    <br>MPI_REAL4<br>MPI_REAL8</font>
    </b></tt></td><td>real<br>real*2<br>real*4<br>real*8</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_DOUBLE</b></tt></td>
    <td>double</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_DOUBLE_PRECISION</b></tt></td>
    <td>double precision</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_LONG_DOUBLE</b></tt></td>
    <td>long double</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_C_COMPLEX<br>MPI_C_FLOAT_COMPLEX</b></tt></td>
    <td>float _Complex</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_COMPLEX</b></tt></td>
    <td>complex</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_C_DOUBLE_COMPLEX</b></tt></td>
    <td>double _Complex</td>
    <td bgcolor="#FOF5FE"><tt><b><font color="gray">MPI_DOUBLE_COMPLEX</font></b></tt></td>
    <td>double complex</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_C_LONG_DOUBLE_COMPLEX</b></tt></td>
    <td>long double _Complex</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_C_BOOL</b></tt></td>
    <td>_Bool</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_LOGICAL</b></tt></td>
    <td>logical</td>

</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_INT8_T 
<br>MPI_INT16_T<br>MPI_INT32_T <br>MPI_INT64_T</b></tt></td>
    <td>int8_t<br>int16_t<br>int32_t <br>int64_t</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UINT8_T 
<br>MPI_UINT16_T <br>MPI_UINT32_T <br>MPI_UINT64_T </b></tt></td>
    <td>uint8_t<br>uint16_t<br>uint32_t<br>uint64_t</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_BYTE</b></tt></td>
    <td>8 binary digits </td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_BYTE</b></tt></td>        
    <td>8 binary digits </td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_PACKED</b></tt></td>
    <td>data packed or unpacked with MPI_Pack()/
        MPI_Unpack</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_PACKED</b></tt></td>
    <td>data packed or unpacked with MPI_Pack()/
        MPI_Unpack</td>
</tr></tbody></table>

<p><strong>Notes:</strong></p>
<ul>
<li>程序员还可以创建自己的数据类型（请参阅<a href="https://computing.llnl.gov/tutorials/mpi/#Derived_Data_Types" target="_blank" rel="noopener">派生数据类型</a>）。</li>
<li>MPI_BYTE和MPI_PACKED与标准C或Fortran类型不对应。<li>Types shown in <font color="gray"><b>GRAY FONT</b></font> are recommended if
      possible.  
  </li></li>
<li>一些实现可能包括其他基本数据类型（MPI_LOGICAL2，MPI_COMPLEX32等）。 检查MPI头文件。</li>
</ul>
<h3 id="Destination"><a href="#Destination" class="headerlink" title="Destination"></a>Destination</h3><p>发送例程的参数，指示应在何处传递消息。 指定为接收进程的rank。</p>
<h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><p>接收例程的参数，指示消息的始发进程。 指定为发送进程的rank。 可以将其设置为通配符MPI_ANY_SOURCE，以接收来自任何任务的消息。</p>
<h3 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h3><p>程序员分配的用于标识消息的任意非负整数。 发送和接收操作应与消息标签匹配。 对于接收操作，通配符MPI_ANY_TAG可以用于接收任何消息，而不管其标签如何。 MPI标准保证可以将整数0-32767用作标记，但是大多数实现允许的范围远大于此范围。</p>
<h3 id="Communicator"><a href="#Communicator" class="headerlink" title="Communicator"></a>Communicator</h3><p>指示通信上下文或源或目标字段对其有效的进程集。 除非程序员明确创建新的通信器，否则通常使用预定义的通信器MPI_COMM_WORLD。</p>
<h3 id="Status"><a href="#Status" class="headerlink" title="Status"></a>Status</h3><p>对于接收操作，指示消息的来源和消息的标签。 在C语言中，此参数是指向预定义结构MPI_Status（例如stat.MPI_SOURCE stat.MPI_TAG）的指针。 在Fortran中，它是大小为MPI_STATUS_SIZE（例如stat（MPI_SOURCE）stat（MPI_TAG））的整数数组。 此外，可以通过MPI_Get_count例程从Status获得所接收的实际字节数。 如果稍后将查询消息的来源，标签或大小，则可以替换常量MPI_STATUS_IGNORE和MPI_STATUSES_IGNORE。</p>
<h3 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h3><p>由非阻塞发送和接收操作使用。 由于非阻塞操作可能会在获得请求的系统缓冲区空间之前返回，因此系统会发出唯一的“请求编号”。 程序员稍后（在WAIT类型的例程中）使用此系统分配的“句柄”来确定非阻塞操作的完成。 在C语言中，此参数是指向预定义结构MPI_Request的指针。 在Fortran中，它是整数。</p>
<h2 id="Blocking-Message-Passing-Routines"><a href="#Blocking-Message-Passing-Routines" class="headerlink" title="Blocking Message Passing Routines"></a>Blocking Message Passing Routines</h2><p>下面介绍了更常用的MPI阻塞消息传递例程。</p>
<h3 id="MPI-Send"><a href="#MPI-Send" class="headerlink" title="MPI_Send"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Send.txt" target="_blank" rel="noopener">MPI_Send</a></h3><p>基本阻塞发送操作。 例程仅在发送任务中的应用程序缓冲区可供重用之后才返回。 注意，该例程可以在不同的系统上以不同的方式实现。 MPI标准允许使用系统缓冲区，但不需要使用它。 一些实现可能实际上使用同步发送（在下面讨论）来实现基本的阻塞发送。</p>
<p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Send (&amp;buf,count,datatype,dest,tag,comm)  <br>
    MPI_SEND (buf,count,datatype,dest,tag,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p>

<h3 id="MPI-Recv"><a href="#MPI-Recv" class="headerlink" title="MPI_Recv"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Recv.txt" target="_blank" rel="noopener">MPI_Recv</a></h3><p>接收消息并阻塞，直到接收任务中的应用程序缓冲区中有所需数据为止。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Recv (&amp;buf,count,datatype,source,tag,comm,&amp;status) <br> 
    MPI_RECV (buf,count,datatype,source,tag,comm,status,ierr) 
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="MPI-Ssend"><a href="#MPI-Ssend" class="headerlink" title="MPI_Ssend "></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Ssend.txt" target="_blank" rel="noopener">MPI_Ssend </a></h3><p>同步阻塞发送：发送消息并进行阻塞，直到发送任务中的应用程序缓冲区可供重新使用且目标进程已开始接收消息为止。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Ssend (&amp;buf,count,datatype,dest,tag,comm)  <br>
    MPI_SSEND (buf,count,datatype,dest,tag,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="MPI-Sendrecv"><a href="#MPI-Sendrecv" class="headerlink" title="MPI_Sendrecv"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Sendrecv.txt" target="_blank" rel="noopener">MPI_Sendrecv</a></h3><p>在阻塞之前发送一个消息并发布一个接收。然后将一直阻塞直到发送应用程序缓冲区可以自由重用和接收应用程序缓冲区包含接收到的消息。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Sendrecv (&amp;sendbuf,sendcount,sendtype,dest,sendtag,  <br>
        <font color="#FFFFFF">......</font> 
                 &amp;recvbuf,recvcount,recvtype,source,recvtag,   <br>
        <font color="#FFFFFF">......</font> 
                 comm,&amp;status)   <br>
    MPI_SENDRECV (sendbuf,sendcount,sendtype,dest,sendtag,  <br> 
        <font color="#FFFFFF">......</font> 
                 recvbuf,recvcount,recvtype,source,recvtag,  <br>
        <font color="#FFFFFF">......</font> 
                 comm,status,ierr)
    </b></tt></nobr><p>
</p></td></tr><tr></tr></tbody></table>

<h3 id="MPI-Wait"><a href="#MPI-Wait" class="headerlink" title="MPI_Wait"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Wait.txt" target="_blank" rel="noopener">MPI_Wait</a></h3><h3 id="MPI-Waitany"><a href="#MPI-Waitany" class="headerlink" title="MPI_Waitany"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitany.txt" target="_blank" rel="noopener">MPI_Waitany</a></h3><h3 id="MPI-Waitall"><a href="#MPI-Waitall" class="headerlink" title="MPI_Waitall"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitall.txt" target="_blank" rel="noopener">MPI_Waitall</a></h3><h3 id="MPI-Waitsome"><a href="#MPI-Waitsome" class="headerlink" title="MPI_Waitsome"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitsome.txt" target="_blank" rel="noopener">MPI_Waitsome</a></h3><p>MPI_Wait会阻塞，直到指定的非阻塞发送或接收操作完成为止。 对于多个非阻塞操作，程序员可以指定任何，全部或部分完成。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Wait     (&amp;request,&amp;status)  <br>
    MPI_Waitany  (count,&amp;array_of_requests,&amp;index,&amp;status)  <br>
    MPI_Waitall  (count,&amp;array_of_requests,&amp;array_of_statuses)  <br>
    MPI_Waitsome (incount,&amp;array_of_requests,&amp;outcount,  <br>
        <font color="#FFFFFF">......</font> 
        &amp;array_of_offsets, &amp;array_of_statuses)  <br>
    MPI_WAIT     (request,status,ierr)  <br>
    MPI_WAITANY  (count,array_of_requests,index,status,ierr)  <br>
    MPI_WAITALL  (count,array_of_requests,array_of_statuses,  <br>
        <font color="#FFFFFF">......</font> 
                 ierr)  <br>
    MPI_WAITSOME (incount,array_of_requests,outcount,  <br>
        <font color="#FFFFFF">......</font> 
                 array_of_offsets, array_of_statuses,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="MPI-Probe"><a href="#MPI-Probe" class="headerlink" title="MPI_Probe"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Probe.txt" target="_blank" rel="noopener">MPI_Probe</a></h3><p>对消息执行阻止测试。 “通配符” MPI_ANY_SOURCE和MPI_ANY_TAG可用于测试来自任何来源或带有任何标签的消息。 对于C例程，实际的源和标签将在状态结构中作为status.MPI_SOURCE和status.MPI_TAG返回。 对于Fortran例程，它们将以整数数组status（MPI_SOURCE）和status（MPI_TAG）返回。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Probe (source,tag,comm,&amp;status)  <br>
    MPI_PROBE (source,tag,comm,status,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="MPI-Get-count"><a href="#MPI-Get-count" class="headerlink" title="MPI_Get_count "></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_count.txt" target="_blank" rel="noopener">MPI_Get_count </a></h3><p>返回接收到的数据类型的元素的来源，标签和数量。 可以与阻塞和非阻塞接收操作一起使用。 对于C例程，实际的源和标签将在状态结构中作为status.MPI_SOURCE和status.MPI_TAG返回。 对于Fortran例程，它们将以整数数组status（MPI_SOURCE）和status（MPI_TAG）返回。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Get_count (&amp;status,datatype,&amp;count)  <br>
    MPI_GET_COUNT (status,datatype,count,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h2 id="Examples-Blocking-Message-Passing-Routines"><a href="#Examples-Blocking-Message-Passing-Routines" class="headerlink" title="Examples: Blocking Message Passing Routines"></a>Examples: Blocking Message Passing Routines</h2><ul>
<p>
Task 0 pings task 1 and awaits return ping
</p><p>

<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Blocking Message Passing Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#df4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;

<p>   main(int argc, char *argv[])  {<br>   int numtasks, rank, dest, source, rc, count, tag=1;<br>   char inmsg, outmsg=’x’;<br>   <font color="#DF4442">MPI_Status Stat</font>;   <font color="#AAAAAA">// required variable for receive routines</font></p>
<p>   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);<br>   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);<br>   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);</p>
<p>   <font color="#AAAAAA">// task 0 sends to task 1 and waits to receive a return message</font><br>   if (rank == 0) {<br>     dest = 1;<br>     source = 1;<br>     <font color="#DF4442">MPI_Send</font>(&amp;outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);<br>     <font color="#DF4442">MPI_Recv</font>(&amp;inmsg, 1, MPI_CHAR, source, tag, MPI_COMM_WORLD, &amp;Stat);<br>     } </p>
<p>   <font color="#AAAAAA">// task 1 waits for task 0 message then returns a message</font><br>   else if (rank == 1) {<br>     dest = 0;<br>     source = 0;<br>     <font color="#DF4442">MPI_Recv</font>(&amp;inmsg, 1, MPI_CHAR, source, tag, MPI_COMM_WORLD, &amp;Stat);<br>     <font color="#DF4442">MPI_Send</font>(&amp;outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);<br>     }</p>
<p>   <font color="#AAAAAA">// query recieve Stat variable and print message details</font><br>   <font color="#DF4442">MPI_Get_count</font>(&amp;Stat, MPI_CHAR, &amp;count);<br>   printf(“Task %d: Received %d char(s) from task %d with tag %d \n”,<br>          rank, count, Stat.MPI_SOURCE, Stat.MPI_TAG);</p>
<p>   <font color="#DF4442">MPI_Finalize</font>();<br>   }</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<p><br><br><br></p>
<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Blocking Message Passing Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program ping
   include <font color="#DF4442">'mpif.h'</font>

<p>   integer numtasks, rank, dest, source, count, tag, ierr<br>   integer <font color="#DF4442">stat(MPI_STATUS_SIZE)</font>   <font color="#AAAAAA">! required variable for receive routines</font><br>   character inmsg, outmsg<br>   outmsg = ‘x’<br>   tag = 1</p>
<p>   call <font color="#DF4442">MPI_INIT</font>(ierr)<br>   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)<br>   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)</p>
<p>   <font color="#AAAAAA">! task 0 sends to task 1 and waits to receive a return message</font><br>   if (rank .eq. 0) then<br>      dest = 1<br>      source = 1<br>      call <font color="#DF4442">MPI_SEND</font>(outmsg, 1, MPI_CHARACTER, dest, tag, MPI_COMM_WORLD, ierr)<br>      call <font color="#DF4442">MPI_RECV</font>(inmsg, 1, MPI_CHARACTER, source, tag, MPI_COMM_WORLD, stat, ierr)</p>
<p>   <font color="#AAAAAA">! task 1 waits for task 0 message then returns a message</font><br>   else if (rank .eq. 1) then<br>      dest = 0<br>      source = 0<br>      call <font color="#DF4442">MPI_RECV</font>(inmsg, 1, MPI_CHARACTER, source, tag, MPI_COMM_WORLD, stat, err)<br>      call <font color="#DF4442">MPI_SEND</font>(outmsg, 1, MPI_CHARACTER, dest, tag, MPI_COMM_WORLD, err)<br>   endif</p>
<p>   <font color="#AAAAAA">! query recieve Stat variable and print message details</font><br>   call <font color="#DF4442">MPI_GET_COUNT</font>(stat, MPI_CHARACTER, count, ierr)<br>   print *, ‘Task ‘,rank,’: Received’, count, ‘char(s) from task’, &amp;<br>            stat(MPI_SOURCE), ‘with tag’,stat(MPI_TAG)</p>
<p>   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)</p>
<p>   end</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->
</p></ul>

<h2 id="非阻塞消息传递例程"><a href="#非阻塞消息传递例程" class="headerlink" title="非阻塞消息传递例程"></a>非阻塞消息传递例程</h2><p>下面描述了更常用的MPI非阻塞消息传递例程。</p>
<h3 id="MPI-Isend"><a href="#MPI-Isend" class="headerlink" title="MPI_Isend"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Isend.txt" target="_blank" rel="noopener">MPI_Isend</a></h3><p>标识内存中用作发送缓冲区的区域。程序立即继续进行，而不等待从应用程序缓冲区复制消息。返回一个通信请求句柄来处理挂起的消息状态。直到随后调用MPI_Wait或MPI_Test表明非阻塞发送已经完成,程序才可以修改应用程序缓冲区。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Isend (&amp;buf,count,datatype,dest,tag,comm,&amp;request) <br>
    MPI_ISEND (buf,count,datatype,dest,tag,comm,request,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="MPI-Irecv"><a href="#MPI-Irecv" class="headerlink" title="MPI_Irecv"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Irecv.txt" target="_blank" rel="noopener">MPI_Irecv</a></h3><p>标识内存中用作接收缓冲区的区域。程序将立即继续，而无需实际等待消息被接收并复制到应用程序缓冲区中。返回一个通信请求句柄来处理挂起的消息状态。程序必须使用调用MPI_Wait或MPI_Test来确定非阻塞接收操作何时完成，请求的消息在应用程序缓冲区中可用。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Irecv (&amp;buf,count,datatype,source,tag,comm,&amp;request) <br>
    MPI_IRECV (buf,count,datatype,source,tag,comm,request,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="MPI-Issend"><a href="#MPI-Issend" class="headerlink" title="MPI_Issend"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Issend.txt" target="_blank" rel="noopener">MPI_Issend</a></h3><p>非阻塞同步发送。与MPI_Isend()类似，除了MPI_Wait()或MPI_Test()表示目标进程何时收到消息。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Issend (&amp;buf,count,datatype,dest,tag,comm,&amp;request) <br>
    MPI_ISSEND (buf,count,datatype,dest,tag,comm,request,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="MPI-Test"><a href="#MPI-Test" class="headerlink" title="MPI_Test"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Test.txt" target="_blank" rel="noopener">MPI_Test</a></h3><h3 id="MPI-Testany"><a href="#MPI-Testany" class="headerlink" title="MPI_Testany"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Testany.txt" target="_blank" rel="noopener">MPI_Testany</a></h3><h3 id="MPI-Testall"><a href="#MPI-Testall" class="headerlink" title="MPI_Testall"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Testall.txt" target="_blank" rel="noopener">MPI_Testall</a></h3><h3 id="MPI-Testsome"><a href="#MPI-Testsome" class="headerlink" title="MPI_Testsome"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Testsome.txt" target="_blank" rel="noopener">MPI_Testsome</a></h3><p>MPI测试检查指定的非阻塞发送或接收操作的状态。如果操作已经完成，“flag”参数将返回逻辑true(1)，如果没有，则返回逻辑false(0)。对于多个非阻塞操作，程序员可以指定任意、全部或部分补全。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Test     (&amp;request,&amp;flag,&amp;status) <br>
    MPI_Testany  (count,&amp;array_of_requests,&amp;index,&amp;flag,&amp;status)<br>
    MPI_Testall  (count,&amp;array_of_requests,&amp;flag,&amp;array_of_statuses)<br>
    MPI_Testsome (incount,&amp;array_of_requests,&amp;outcount,<br>
        <font color="#FFFFFF">......</font> 
                 &amp;array_of_offsets, &amp;array_of_statuses)<br>
    MPI_TEST     (request,flag,status,ierr)<br>
    MPI_TESTANY  (count,array_of_requests,index,flag,status,ierr)<br>
    MPI_TESTALL  (count,array_of_requests,flag,array_of_statuses,ierr)<br>
    MPI_TESTSOME (incount,array_of_requests,outcount,<br>
        <font color="#FFFFFF">......</font> 
                 array_of_offsets, array_of_statuses,ierr)<br>
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="MPI-Iprobe"><a href="#MPI-Iprobe" class="headerlink" title="MPI_Iprobe"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Iprobe.txt" target="_blank" rel="noopener">MPI_Iprobe</a></h3><p>对消息执行非阻塞测试。“通配符”MPI_ANY_SOURCE和MPI_ANY_TAG可以用于测试来自任何源或带有任何标记的消息。如果消息已经到达，那么整数“flag”参数将返回逻辑true(1)，如果没有到达，则返回逻辑false(0)。对于C例程，实际的源和标记将在状态结构中作为status返回MPI_SOURCE和status.MPI_TAG。对于Fortran例程，它们将以整数数组状态(MPI源)和状态(MPI标记)的形式返回。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Iprobe (source,tag,comm,&amp;flag,&amp;status)<br>
    MPI_IPROBE (source,tag,comm,flag,status,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="示例-非阻塞消息传递例程"><a href="#示例-非阻塞消息传递例程" class="headerlink" title="示例:非阻塞消息传递例程"></a>示例:非阻塞消息传递例程</h3><p>环拓扑中的最近邻交换</p>
<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Non-blocking Message Passing Example</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;

<p>   main(int argc, char *argv[])  {<br>   int numtasks, rank, next, prev, buf[2], tag1=1, tag2=2;<br>   <font color="#DF4442">MPI_Request reqs[4]</font>;   <font color="#AAAAAA">// required variable for non-blocking calls</font><br>   <font color="#DF4442">MPI_Status stats[4]</font>;   <font color="#AAAAAA">// required variable for Waitall routine</font></p>
<p>   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);<br>   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);<br>   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);</p>
<p>   <font color="#AAAAAA">// determine left and right neighbors</font><br>   prev = rank-1;<br>   next = rank+1;<br>   if (rank == 0)  prev = numtasks - 1;<br>   if (rank == (numtasks - 1))  next = 0;</p>
<p>   <font color="#AAAAAA">// post non-blocking receives and sends for neighbors</font><br>   <font color="#DF4442">MPI_Irecv</font>(&amp;buf[0], 1, MPI_INT, prev, tag1, MPI_COMM_WORLD, &amp;reqs[0]);<br>   <font color="#DF4442">MPI_Irecv</font>(&amp;buf[1], 1, MPI_INT, next, tag2, MPI_COMM_WORLD, &amp;reqs[1]);</p>
<p>   <font color="#DF4442">MPI_Isend</font>(&amp;rank, 1, MPI_INT, prev, tag2, MPI_COMM_WORLD, &amp;reqs[2]);<br>   <font color="#DF4442">MPI_Isend</font>(&amp;rank, 1, MPI_INT, next, tag1, MPI_COMM_WORLD, &amp;reqs[3]);</p>
<p>   <font color="#AAAAAA">   // do some work while sends/receives progress in background</font></p>
<p>   <font color="#AAAAAA">// wait for all non-blocking operations to complete</font><br>   <font color="#DF4442">MPI_Waitall</font>(4, reqs, stats);</p>
<p>   <font color="#AAAAAA">   // continue - do more work</font></p>
<p>   <font color="#DF4442">MPI_Finalize</font>();<br>   }</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<h2 id="集体通信例程"><a href="#集体通信例程" class="headerlink" title="集体通信例程"></a>集体通信例程</h2><h4 id="集体作业的类型"><a href="#集体作业的类型" class="headerlink" title="集体作业的类型"></a>集体作业的类型</h4><ul>
<li><strong>同步</strong>——进程等待，直到组中的所有成员都达到同步点。</li>
<li><strong>数据传送</strong>——广播(broadcast)，散布(scatter)和收集(gather),all to all. </li>
<li><strong>集合计算(归约)</strong>——组中的一个成员从其他成员那里收集数据，并对该数据执行操作(最小、最大、加、乘等)。</li>
</ul>
<h4 id="Scope"><a href="#Scope" class="headerlink" title="Scope:"></a>Scope:</h4><ul>
<li>集合通信例行程序必须包括通信者范围内的所有进程。<ul>
<li>默认情况下，所有进程都是通信器MPI_COMM_WORLD中的成员。</li>
<li>程序员可以定义其他通信器。有关详细信息，请参阅<a href="https://computing.llnl.gov/tutorials/mpi/#Group_Management_Routines" target="_blank" rel="noopener"> Group and Communicator Management Routines</a>部分。</li>
</ul>
</li>
<li>如果通信器中甚至有一个任务不参与，就可能发生意外的行为，包括程序失败。</li>
<li>确保通信器中的所有进程都参与到任何集体操作中是程序员的责任。</li>
</ul>
<h4 id="编程注意事项和限制"><a href="#编程注意事项和限制" class="headerlink" title="编程注意事项和限制"></a>编程注意事项和限制</h4><ul>
<li>集体通信例程不接受消息标签参数。</li>
<li>进程子集内的集体操作是通过首先将子集划分为新组，然后将新组附加到新通信器来完成的((discussed in the <a href="https://computing.llnl.gov/tutorials/mpi/#Group_Management_Routines" target="_blank" rel="noopener">Group and Communicator Management Routines</a> section). )</li>
<li>只能与MPI预定义的数据类型一起使用——而不能与MPI<a href="https://computing.llnl.gov/tutorials/mpi/#Derived_Data_Types" target="_blank" rel="noopener">派生的数据类型</a>一起使用。</li>
<li>MPI-2扩展了大多数集合操作，允许在通信器之间移动数据(这里没有介绍)。</li>
<li>对于MPI-3，集合操作可以是阻塞的，也可以是非阻塞的。本教程只介绍阻塞操作。</li>
</ul>
<h3 id="集合通信例程"><a href="#集合通信例程" class="headerlink" title="集合通信例程"></a>集合通信例程</h3><h4 id="MPI-Barrier"><a href="#MPI-Barrier" class="headerlink" title="MPI_Barrier"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Barrier.txt" target="_blank" rel="noopener">MPI_Barrier</a></h4><p>同步操作。在组中创建barrier同步。每个任务在到达MPI_Barrier调用时都会阻塞，直到组中的所有任务都到达相同的MPI_Barrier调用为止。然后所有的任务都可以继续进行。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Barrier (comm)<br>
    MPI_BARRIER (comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Bcast"><a href="#MPI-Bcast" class="headerlink" title="MPI_Bcast"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Bcast.txt" target="_blank" rel="noopener">MPI_Bcast</a></h4><p>数据移动操作。从rank为“root”的进程向组中的所有其他进程广播(发送)消息。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Bcast (&amp;buffer,count,datatype,root,comm)   <br>
    MPI_BCAST (buffer,count,datatype,root,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Scatter"><a href="#MPI-Scatter" class="headerlink" title="MPI_Scatter"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Scatter.txt" target="_blank" rel="noopener">MPI_Scatter</a></h4><p>数据移动操作。将来自单个源任务的不同消息分发到组中的每个任务。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Scatter (&amp;sendbuf,sendcnt,sendtype,&amp;recvbuf,   <br>
        <font color="#FFFFFF">......</font> 
                recvcnt,recvtype,root,comm)  <br>
    MPI_SCATTER (sendbuf,sendcnt,sendtype,recvbuf,  <br> 
        <font color="#FFFFFF">......</font> 
                recvcnt,recvtype,root,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Gather"><a href="#MPI-Gather" class="headerlink" title="MPI_Gather"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Gather.txt" target="_blank" rel="noopener">MPI_Gather</a></h4><p>数据移动操作。将组中每个任务的不同消息收集到单个目标任务。这个例程是MPI_Scatter的反向操作。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Gather (&amp;sendbuf,sendcnt,sendtype,&amp;recvbuf,  <br>
        <font color="#FFFFFF">......</font> 
               recvcount,recvtype,root,comm)  <br>
    MPI_GATHER (sendbuf,sendcnt,sendtype,recvbuf,  <br>
        <font color="#FFFFFF">......</font> 
               recvcount,recvtype,root,comm,ierr)  
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Allgather"><a href="#MPI-Allgather" class="headerlink" title="MPI_Allgather"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Allgather.txt" target="_blank" rel="noopener">MPI_Allgather</a></h4><p>数据移动操作。数据与组中所有任务的连接。组中的每个任务实际上在组内执行一对所有的广播操作。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Allgather (&amp;sendbuf,sendcount,sendtype,&amp;recvbuf,  <br>
        <font color="#FFFFFF">......</font> 
                  recvcount,recvtype,comm) <br>
    MPI_ALLGATHER (sendbuf,sendcount,sendtype,recvbuf, <br> 
        <font color="#FFFFFF">......</font> 
                  recvcount,recvtype,comm,info)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Reduce"><a href="#MPI-Reduce" class="headerlink" title=" MPI_Reduce "></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Reduce.txt" target="_blank" rel="noopener"> MPI_Reduce </a></h4><p>集体计算操作。对组中的所有任务应用reduce操作，并将结果放在一个任务中。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Reduce (&amp;sendbuf,&amp;recvbuf,count,datatype,op,root,comm) <br>
    MPI_REDUCE (sendbuf,recvbuf,count,datatype,op,root,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<p>下面显示了预定义的MPI精简操作。用户还可以使用<a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Op_create.txt" target="_blank" rel="noopener">MPI_Op_create</a>例程定义自己的reduce函数。</p>
<table width="90%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="TOP">
<th colspan="2">MPI Reduction Operation</th> 
<th>C Data Types</th>
<th>Fortran Data Type</th>
</tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b> MPI_MAX    
</b></tt></td><td>maximum       
</td><td>integer, float      
</td><td>integer, real, complex  
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_MIN    
</b></tt></td><td>minimum       
</td><td>integer, float      
</td><td>integer, real, complex  
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_SUM    
</b></tt></td><td>sum          
</td><td>integer, float      
</td><td>integer, real, complex  
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_PROD    
</b></tt></td><td>product      
</td><td>integer, float      
</td><td>integer, real, complex  
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_LAND    
</b></tt></td><td>logical AND   
</td><td>integer           
</td><td>logical                 
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_BAND    
</b></tt></td><td>bit-wise AND  
</td><td>integer, MPI_BYTE   
</td><td>integer, MPI_BYTE      
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_LOR    
</b></tt></td><td>logical OR    
</td><td>integer            
</td><td>logical                 
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_BOR    
</b></tt></td><td>bit-wise OR   
</td><td>integer, MPI_BYTE   
</td><td>integer, MPI_BYTE       
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_LXOR    
</b></tt></td><td>logical XOR   
</td><td>integer           
</td><td>logical                 
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_BXOR    
</b></tt></td><td>bit-wise XOR  
</td><td>integer, MPI_BYTE   
</td><td>integer, MPI_BYTE      
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_MAXLOC  
</b></tt></td><td>max value and location
</td><td>float, double and long double         
</td><td>real, complex,double precision        
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_MINLOC  
</b></tt></td><td>min value and location 
</td><td>float, double and long double         
</td><td>real, complex, double precision        
</td></tr></tbody></table>

<ul>
<li>MPI_Reduce手册页中的说明:操作总是被假定为关联的。所有预定义的操作都假定是可交换的。用户可以定义被假定为关联而非交换的操作。归约的“规范”评估顺序由组中进程的rank决定。但是，实现可以利用结合性，或者结合性和交换性来改变计算顺序。这可能会改变非严格结合性和可交换性操作(如浮点加法)的归约结果。强烈建议实现MPI_REDUCE，以便当函数被应用到相同的参数上时，以相同的顺序出现时，可以获得相同的结果。注意，这可能会阻止优化利用处理器的物理位置。</li>
</ul>
<h4 id="MPI-Allreduce"><a href="#MPI-Allreduce" class="headerlink" title=" MPI_Allreduce "></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Allreduce.txt" target="_blank" rel="noopener"> MPI_Allreduce </a></h4><p>集体计算操作+数据移动。应用归约操作并将结果放置到组中的所有任务中。这相当于MPI_Reduce后再进行MPI_Bcast。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Allreduce (&amp;sendbuf,&amp;recvbuf,count,datatype,op,comm) <br>
    MPI_ALLREDUCE (sendbuf,recvbuf,count,datatype,op,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Reduce-scatter"><a href="#MPI-Reduce-scatter" class="headerlink" title=" MPI_Reduce_scatter "></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Reduce_scatter.txt" target="_blank" rel="noopener"> MPI_Reduce_scatter </a></h4><p>集体计算操作+数据移动。首先对组中所有任务的向量进行element-wise的归约。接下来，结果向量被分割成不相交的片段并分布在各个任务中。这相当于在MPI_Reduce之后执行MPI_Scatter操作。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Reduce_scatter (&amp;sendbuf,&amp;recvbuf,recvcount,datatype, <br>
        <font color="#FFFFFF">......</font>
         op,comm) <br>
    MPI_REDUCE_SCATTER (sendbuf,recvbuf,recvcount,datatype, <br>
        <font color="#FFFFFF">......</font>
         op,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Alltoall"><a href="#MPI-Alltoall" class="headerlink" title=" MPI_Alltoall "></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Alltoall.txt" target="_blank" rel="noopener"> MPI_Alltoall </a></h4><p>数据移动操作。组中的每个任务执行scatter操作，按照索引的顺序向组中的所有任务发送不同的消息。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Alltoall (&amp;sendbuf,sendcount,sendtype,&amp;recvbuf, <br>
        <font color="#FFFFFF">......</font>
                 recvcnt,recvtype,comm) <br>
    MPI_ALLTOALL (sendbuf,sendcount,sendtype,recvbuf, <br>
        <font color="#FFFFFF">......</font>
                 recvcnt,recvtype,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Scan"><a href="#MPI-Scan" class="headerlink" title=" MPI_Scan "></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Scan.txt" target="_blank" rel="noopener"> MPI_Scan </a></h4><p>对整个任务组的归约操作执行扫描操作。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Scan (&amp;sendbuf,&amp;recvbuf,count,datatype,op,comm) <br>
    MPI_SCAN (sendbuf,recvbuf,count,datatype,op,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="例子-集体通信"><a href="#例子-集体通信" class="headerlink" title="例子:集体通信"></a>例子:集体通信</h3><p>对数组的行执行分散操作</p>
<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Collective Communications Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define SIZE 4

<p>   main(int argc, char *argv[])  {<br>   int numtasks, rank, sendcount, recvcount, source;<br>   float sendbuf[SIZE][SIZE] = {<br>     {1.0, 2.0, 3.0, 4.0},<br>     {5.0, 6.0, 7.0, 8.0},<br>     {9.0, 10.0, 11.0, 12.0},<br>     {13.0, 14.0, 15.0, 16.0}  };<br>   float recvbuf[SIZE];</p>
<p>   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);<br>   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);<br>   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);</p>
<p>   if (numtasks == SIZE) {<br>     <font color="#AAAAAA">// define source task and elements to send/receive, then perform collective scatter</font><br>     source = 1;<br>     sendcount = SIZE;<br>     recvcount = SIZE;<br>     <font color="#DF4442">MPI_Scatter</font>(sendbuf,sendcount,MPI_FLOAT,recvbuf,recvcount,<br>                 MPI_FLOAT,source,MPI_COMM_WORLD);</p>
<pre><code>printf(&quot;rank= %d  Results: %f %f %f %f\n&quot;,rank,recvbuf[0],
       recvbuf[1],recvbuf[2],recvbuf[3]);
}</code></pre><p>   else<br>     printf(“Must specify %d processors. Terminating.\n”,SIZE);</p>
<p>   <font color="#DF4442">MPI_Finalize</font>();<br>   }</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Collective Communications Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program scatter
   include <font color="#DF4442">'mpif.h'</font>

<p>   integer SIZE<br>   parameter(SIZE=4)<br>   integer numtasks, rank, sendcount, recvcount, source, ierr<br>   real*4 sendbuf(SIZE,SIZE), recvbuf(SIZE)</p>
<p>   <font color="#AAAAAA">! Fortran stores this array in column major order, so the<br>   ! scatter will actually scatter columns, not rows.</font><br>   data sendbuf /1.0, 2.0, 3.0, 4.0, &amp;<br>                 5.0, 6.0, 7.0, 8.0, &amp;<br>                 9.0, 10.0, 11.0, 12.0, &amp;<br>                 13.0, 14.0, 15.0, 16.0 /</p>
<p>   call <font color="#DF4442">MPI_INIT</font>(ierr)<br>   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)<br>   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)</p>
<p>   if (numtasks .eq. SIZE) then<br>      <font color="#AAAAAA">! define source task and elements to send/receive, then perform collective scatter</font><br>      source = 1<br>      sendcount = SIZE<br>      recvcount = SIZE<br>      call <font color="#DF4442">MPI_SCATTER</font>(sendbuf, sendcount, MPI_REAL, recvbuf, recvcount, MPI_REAL, &amp;<br>                       source, MPI_COMM_WORLD, ierr)</p>
<pre><code>print *, &apos;rank= &apos;,rank,&apos; Results: &apos;,recvbuf </code></pre><p>   else<br>      print *, ‘Must specify’,SIZE,’ processors.  Terminating.’<br>   endif</p>
<p>   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)</p>
<p>   end</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<h2 id="派生数据类型"><a href="#派生数据类型" class="headerlink" title="派生数据类型"></a>派生数据类型</h2><ul>
<li>如前所述，MPI预定义了它的基本数据类型</li>
</ul>
<table width="90%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr><th colspan="2">C Data Types</th>
<th>Fortran Data Types</th>
</tr><tr valign="top">
<td width="33%"><pre>MPI_CHAR
MPI_WCHAR
MPI_SHORT
MPI_INT
MPI_LONG
MPI_LONG_LONG_INT 
MPI_LONG_LONG          
MPI_SIGNED_CHAR
MPI_UNSIGNED_CHAR
MPI_UNSIGNED_SHORT
MPI_UNSIGNED_LONG
MPI_UNSIGNED
MPI_FLOAT
MPI_DOUBLE
MPI_LONG_DOUBLE
</pre></td>
<td width="33%"><pre>MPI_C_COMPLEX
MPI_C_FLOAT_COMPLEX
MPI_C_DOUBLE_COMPLEX
MPI_C_LONG_DOUBLE_COMPLEX          
MPI_C_BOOL
MPI_LOGICAL
MPI_C_LONG_DOUBLE_COMPLEX      
MPI_INT8_T 
MPI_INT16_T
MPI_INT32_T 
MPI_INT64_T          
MPI_UINT8_T 
MPI_UINT16_T 
MPI_UINT32_T 
MPI_UINT64_T
MPI_BYTE
MPI_PACKED
</pre></td>
<td width="33%"><pre>MPI_CHARACTER
MPI_INTEGER
MPI_INTEGER1 
MPI_INTEGER2
MPI_INTEGER4
MPI_REAL
MPI_REAL2 
MPI_REAL4
MPI_REAL8
MPI_DOUBLE_PRECISION
MPI_COMPLEX
MPI_DOUBLE_COMPLEX
MPI_LOGICAL
MPI_BYTE
MPI_PACKED
</pre></td>
</tr></tbody></table>

<ul>
<li>MPI还提供了根据MPI基本数据类型序列定义自己的数据结构的工具。这种用户定义的结构称为派生数据类型。</li>
<li>基本数据类型是连续的。派生数据类型允许您以方便的方式指定非连续数据，并将其视为连续数据。</li>
<li>MPI提供了几种构造派生数据类型的方法<ul>
<li>连续的（Contiguous）</li>
<li>向量（Vector）</li>
<li>Indexed</li>
<li>Struct</li>
</ul>
</li>
</ul>
<h3 id="派生数据类型例程"><a href="#派生数据类型例程" class="headerlink" title="派生数据类型例程"></a>派生数据类型例程</h3><h4 id="MPI-Type-contiguous"><a href="#MPI-Type-contiguous" class="headerlink" title=" MPI_Type_contiguous"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_contiguous.txt" target="_blank" rel="noopener"> MPI_Type_contiguous</a></h4><p>最简单的构造函数。通过对现有数据类型进行计数复制，生成新的数据类型。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_contiguous (count,oldtype,&amp;newtype) <br>
    MPI_TYPE_CONTIGUOUS (count,oldtype,newtype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Type-vector"><a href="#MPI-Type-vector" class="headerlink" title="MPI_Type_vector"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_vector.txt" target="_blank" rel="noopener">MPI_Type_vector</a></h4><h4 id="MPI-Type-hvector"><a href="#MPI-Type-hvector" class="headerlink" title="MPI_Type_hvector"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_hvector.txt" target="_blank" rel="noopener">MPI_Type_hvector</a></h4><p>类似于连续，但允许在位移中有规则的间隙(stride)。MPI_Type_hvector与MPI_Type_vector相同，不同的是跨步是以字节为单位指定的。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_vector (count,blocklength,stride,oldtype,&amp;newtype)<br> 
    MPI_TYPE_VECTOR (count,blocklength,stride,oldtype,newtype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Type-indexed"><a href="#MPI-Type-indexed" class="headerlink" title="MPI_Type_indexed"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_indexed.txt" target="_blank" rel="noopener">MPI_Type_indexed</a></h4><h4 id="MPI-Type-hindexed"><a href="#MPI-Type-hindexed" class="headerlink" title=" MPI_Type_hindexed "></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_hindexed.txt" target="_blank" rel="noopener"> MPI_Type_hindexed </a></h4><p>提供输入数据类型的位移数组作为新数据类型的映射。MPI_Type_hindexed与MPI_Type_indexed是相同的，除了偏移量是用字节指定的。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_indexed (count,blocklens[],offsets[],old_type,&amp;newtype)<br>
    MPI_TYPE_INDEXED (count,blocklens(),offsets(),old_type,newtype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Type-struct"><a href="#MPI-Type-struct" class="headerlink" title="MPI_Type_struct"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_struct.txt" target="_blank" rel="noopener">MPI_Type_struct</a></h4><p>新的数据类型是根据组件数据类型的完全定义映射形成的。</p>
<p><strong>NOTE:</strong> 该函数在MPI-2.0中不提倡使用，在MPI-3.0中被MPI_Type_creat_struct替代</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_struct (count,blocklens[],offsets[],old_types,&amp;newtype)<br>
    MPI_TYPE_STRUCT (count,blocklens(),offsets(),old_types,newtype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Type-extent"><a href="#MPI-Type-extent" class="headerlink" title="MPI_Type_extent"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_extent.txt" target="_blank" rel="noopener">MPI_Type_extent</a></h4><p>返回指定数据类型的大小(以字节为单位)。对于需要指定字节偏移量的MPI子例程很有用。</p>
<p><strong>NOTE:</strong> 该函数在MPI-2.0中不赞成使用，在MPI-3.0中被MPI_Type_get_extent替换</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_extent (datatype,&amp;extent)<br>
    MPI_TYPE_EXTENT (datatype,extent,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Type-commit"><a href="#MPI-Type-commit" class="headerlink" title="MPI_Type_commit"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_commit.txt" target="_blank" rel="noopener">MPI_Type_commit</a></h4><p>向系统提交新的数据类型。所有用户构造(派生)数据类型都需要。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_commit (&amp;datatype)<br>
    MPI_TYPE_COMMIT (datatype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h4 id="MPI-Type-free"><a href="#MPI-Type-free" class="headerlink" title="MPI_Type_free"></a><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_free.txt" target="_blank" rel="noopener">MPI_Type_free</a></h4><p>释放指定的数据类型对象。如果在循环中创建了许多数据类型对象，那么使用这个例程对于防止内存耗尽尤其重要。</p>
<table width="75%" cellspacing="0" cellpadding="5" border="1">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_free (&amp;datatype)<br>
    MPI_TYPE_FREE (datatype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>

<h3 id="示例-连续派生数据类型"><a href="#示例-连续派生数据类型" class="headerlink" title="示例:连续派生数据类型"></a>示例:连续派生数据类型</h3><p>创建表示数组中的一行的数据类型，并将另一行分发给所有进程。</p>
<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Contiguous Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define SIZE 4

<p>   main(int argc, char *argv[])  {<br>   int numtasks, rank, source=0, dest, tag=1, i;<br>   float a[SIZE][SIZE] =<br>     {1.0, 2.0, 3.0, 4.0,<br>      5.0, 6.0, 7.0, 8.0,<br>      9.0, 10.0, 11.0, 12.0,<br>      13.0, 14.0, 15.0, 16.0};<br>   float b[SIZE];</p>
<p>   <font color="#DF4442">MPI_Status stat</font>;<br>   <font color="#DF4442">MPI_Datatype rowtype</font>;   <font color="#AAAAAA">// required variable</font></p>
<p>   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);<br>   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);<br>   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);</p>
<p>   <font color="#AAAAAA">// create contiguous derived data type</font><br>   <font color="#DF4442">MPI_Type_contiguous</font>(SIZE, MPI_FLOAT, &amp;rowtype);<br>   <font color="#DF4442">MPI_Type_commit</font>(&amp;rowtype);</p>
<p>   if (numtasks == SIZE) {<br>      <font color="#AAAAAA">// task 0 sends one element of rowtype to all tasks</font><br>      if (rank == 0) {<br>         for (i=0; i&lt;numtasks; i++)<br>           <font color="#DF4442">MPI_Send</font>(&amp;a[i][0], 1, rowtype, i, tag, MPI_COMM_WORLD);<br>         }</p>
<pre><code>&lt;font color=&quot;#AAAAAA&quot;&gt;// all tasks receive rowtype data from task 0&lt;/font&gt;
&lt;font color=&quot;#DF4442&quot;&gt;MPI_Recv&lt;/font&gt;(b, SIZE, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &amp;amp;stat);
printf(&quot;rank= %d  b= %3.1f %3.1f %3.1f %3.1f\n&quot;,
       rank,b[0],b[1],b[2],b[3]);
}</code></pre><p>   else<br>      printf(“Must specify %d processors. Terminating.\n”,SIZE);</p>
<p>   <font color="#AAAAAA">// free datatype when done using it</font><br>   <font color="#DF4442">MPI_Type_free</font>(&amp;rowtype);<br>   <font color="#DF4442">MPI_Finalize</font>();<br>   }</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Contiguous Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program contiguous
   include <font color="#DF4442">'mpif.h'</font>

<p>   integer SIZE<br>   parameter(SIZE=4)<br>   integer numtasks, rank, source, dest, tag, i,  ierr<br>   real*4 a(0:SIZE-1,0:SIZE-1), b(0:SIZE-1)<br>   integer <font color="#DF4442">stat(MPI_STATUS_SIZE)</font><br>   integer <font color="#DF4442">columntype</font>   <font color="#AAAAAA">! required variable</font><br>   tag = 1</p>
<p>   <font color="#AAAAAA">! Fortran stores this array in column major order</font><br>   data a  /1.0, 2.0, 3.0, 4.0, &amp;<br>            5.0, 6.0, 7.0, 8.0, &amp;<br>            9.0, 10.0, 11.0, 12.0, &amp;<br>            13.0, 14.0, 15.0, 16.0 /</p>
<p>   call <font color="#DF4442">MPI_INIT</font>(ierr)<br>   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)<br>   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)</p>
<p>   <font color="#AAAAAA">! create contiguous derived data type</font><br>   call <font color="#DF4442">MPI_TYPE_CONTIGUOUS</font>(SIZE, MPI_REAL, columntype, ierr)<br>   call <font color="#DF4442">MPI_TYPE_COMMIT</font>(columntype, ierr)</p>
<p>   if (numtasks .eq. SIZE) then<br>      <font color="#AAAAAA">! task 0 sends one element of columntype to all tasks</font><br>      if (rank .eq. 0) then<br>         do i=0, numtasks-1<br>         call <font color="#DF4442">MPI_SEND</font>(a(0,i), 1, columntype, i, tag, MPI_COMM_WORLD,ierr)<br>         end do<br>      endif</p>
<pre><code>&lt;font color=&quot;#AAAAAA&quot;&gt;! all tasks receive columntype data from task 0&lt;/font&gt;
source = 0
call &lt;font color=&quot;#DF4442&quot;&gt;MPI_RECV&lt;/font&gt;(b, SIZE, MPI_REAL, source, tag, MPI_COMM_WORLD, stat, ierr)
print *, &apos;rank= &apos;,rank,&apos; b= &apos;,b</code></pre><p>   else<br>      print *, ‘Must specify’,SIZE,’ processors.  Terminating.’<br>   endif</p>
<p>   <font color="#AAAAAA">! free datatype when done using it</font><br>   call <font color="#DF4442">MPI_TYPE_FREE</font>(columntype, ierr)<br>   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)</p>
<p>   end</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<p> Sample program output: </p>
 <pre>rank= 0  b= 1.0 2.0 3.0 4.0
rank= 1  b= 5.0 6.0 7.0 8.0
rank= 2  b= 9.0 10.0 11.0 12.0
rank= 3  b= 13.0 14.0 15.0 16.0
</pre>

<h3 id="示例-向量派生的数据类型"><a href="#示例-向量派生的数据类型" class="headerlink" title="示例:向量派生的数据类型"></a>示例:向量派生的数据类型</h3><p>创建表示数组中的列的数据类型，并将不同的列分发给所有进程。</p>
<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Vector Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define SIZE 4

<p>   main(int argc, char *argv[])  {<br>   int numtasks, rank, source=0, dest, tag=1, i;<br>   float a[SIZE][SIZE] =<br>     {1.0, 2.0, 3.0, 4.0,<br>      5.0, 6.0, 7.0, 8.0,<br>      9.0, 10.0, 11.0, 12.0,<br>     13.0, 14.0, 15.0, 16.0};<br>   float b[SIZE]; </p>
<p>   <font color="#DF4442">MPI_Status stat</font>;<br>   <font color="#DF4442">MPI_Datatype columntype</font>;   <font color="#AAAAAA">// required variable</font></p>
<p>   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);<br>   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);<br>   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);</p>
<p>   <font color="#AAAAAA">// create vector derived data type</font><br>   <font color="#DF4442">MPI_Type_vector</font>(SIZE, 1, SIZE, MPI_FLOAT, &amp;columntype);<br>   <font color="#DF4442">MPI_Type_commit</font>(&amp;columntype);</p>
<p>   if (numtasks == SIZE) {<br>      <font color="#AAAAAA">// task 0 sends one element of columntype to all tasks</font><br>      if (rank == 0) {<br>         for (i=0; i&lt;numtasks; i++)<br>            <font color="#DF4442">MPI_Send</font>(&amp;a[0][i], 1, columntype, i, tag, MPI_COMM_WORLD);<br>         }</p>
<pre><code>&lt;font color=&quot;#AAAAAA&quot;&gt;// all tasks receive columntype data from task 0&lt;/font&gt;
&lt;font color=&quot;#DF4442&quot;&gt;MPI_Recv&lt;/font&gt;(b, SIZE, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &amp;amp;stat);
printf(&quot;rank= %d  b= %3.1f %3.1f %3.1f %3.1f\n&quot;,
       rank,b[0],b[1],b[2],b[3]);
}</code></pre><p>   else<br>      printf(“Must specify %d processors. Terminating.\n”,SIZE);</p>
<p>   <font color="#AAAAAA">// free datatype when done using it</font><br>   <font color="#DF4442">MPI_Type_free</font>(&amp;columntype);<br>   <font color="#DF4442">MPI_Finalize</font>();<br>   }</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Vector Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program vector
   include <font color="#DF4442">'mpif.h'</font>

<p>   integer SIZE<br>   parameter(SIZE=4)<br>   integer numtasks, rank, source, dest, tag, i,  ierr<br>   real*4 a(0:SIZE-1,0:SIZE-1), b(0:SIZE-1)<br>   integer <font color="#DF4442">stat(MPI_STATUS_SIZE)</font><br>   integer <font color="#DF4442">rowtype</font>   <font color="#AAAAAA">! required variable</font><br>   tag = 1</p>
<p>   <font color="#AAAAAA">! Fortran stores this array in column major order</font><br>   data a  /1.0, 2.0, 3.0, 4.0, &amp;<br>            5.0, 6.0, 7.0, 8.0,  &amp;<br>            9.0, 10.0, 11.0, 12.0, &amp;<br>            13.0, 14.0, 15.0, 16.0 /</p>
<p>   call <font color="#DF4442">MPI_INIT</font>(ierr)<br>   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)<br>   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)</p>
<p>   <font color="#AAAAAA">! create vector derived data type</font><br>   call <font color="#DF4442">MPI_TYPE_VECTOR</font>(SIZE, 1, SIZE, MPI_REAL, rowtype, ierr)<br>   call <font color="#DF4442">MPI_TYPE_COMMIT</font>(rowtype, ierr)</p>
<p>   if (numtasks .eq. SIZE) then<br>      <font color="#AAAAAA">! task 0 sends one element of rowtype to all tasks</font><br>      if (rank .eq. 0) then<br>         do i=0, numtasks-1<br>         call <font color="#DF4442">MPI_SEND</font>(a(i,0), 1, rowtype, i, tag, MPI_COMM_WORLD, ierr)<br>         end do<br>      endif</p>
<pre><code>&lt;font color=&quot;#AAAAAA&quot;&gt;! all tasks receive rowtype data from task 0&lt;/font&gt;
source = 0
call &lt;font color=&quot;#DF4442&quot;&gt;MPI_RECV&lt;/font&gt;(b, SIZE, MPI_REAL, source, tag, MPI_COMM_WORLD, stat, ierr)
print *, &apos;rank= &apos;,rank,&apos; b= &apos;,b</code></pre><p>   else<br>      print *, ‘Must specify’,SIZE,’ processors.  Terminating.’<br>   endif</p>
<p>   <font color="#AAAAAA">! free datatype when done using it</font><br>   call <font color="#DF4442">MPI_TYPE_FREE</font>(rowtype, ierr)<br>   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)</p>
<p>   end</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<p> Sample program output: </p>
 <pre>rank= 0  b= 1.0 5.0 9.0 13.0
rank= 1  b= 2.0 6.0 10.0 14.0
rank= 2  b= 3.0 7.0 11.0 15.0
rank= 3  b= 4.0 8.0 12.0 16.0
</pre>

<h3 id="示例-索引派生数据类型"><a href="#示例-索引派生数据类型" class="headerlink" title="示例:索引派生数据类型"></a>示例:索引派生数据类型</h3><p>通过提取数组的可变部分来创建数据类型并分发给所有任务。</p>
<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Indexed Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define NELEMENTS 6

<p>   main(int argc, char *argv[])  {<br>   int numtasks, rank, source=0, dest, tag=1, i;<br>   int blocklengths[2], displacements[2];<br>   float a[16] =<br>     {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0,<br>      9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0};<br>   float b[NELEMENTS]; </p>
<p>   <font color="#DF4442">MPI_Status stat</font>;<br>   <font color="#DF4442">MPI_Datatype indextype</font>;   <font color="#AAAAAA">// required variable</font></p>
<p>   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);<br>   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);<br>   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);</p>
<p>   blocklengths[0] = 4;<br>   blocklengths[1] = 2;<br>   displacements[0] = 5;<br>   displacements[1] = 12;</p>
<p>   <font color="#AAAAAA">// create indexed derived data type</font><br>   <font color="#DF4442">MPI_Type_indexed</font>(2, blocklengths, displacements, MPI_FLOAT, &amp;indextype);<br>   <font color="#DF4442">MPI_Type_commit</font>(&amp;indextype);</p>
<p>   if (rank == 0) {<br>     for (i=0; i&lt;numtasks; i++)<br>      <font color="#AAAAAA">// task 0 sends one element of indextype to all tasks</font><br>        <font color="#DF4442">MPI_Send</font>(a, 1, indextype, i, tag, MPI_COMM_WORLD);<br>     }</p>
<p>   <font color="#AAAAAA">// all tasks receive indextype data from task 0</font><br>   <font color="#DF4442">MPI_Recv</font>(b, NELEMENTS, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &amp;stat);<br>   printf(“rank= %d  b= %3.1f %3.1f %3.1f %3.1f %3.1f %3.1f\n”,<br>          rank,b[0],b[1],b[2],b[3],b[4],b[5]);</p>
<p>   <font color="#AAAAAA">// free datatype when done using it</font><br>   <font color="#DF4442">MPI_Type_free</font>(&amp;indextype);<br>   <font color="#DF4442">MPI_Finalize</font>();<br>   }</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Indexed Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program indexed
   include <font color="#DF4442">'mpif.h'</font>

<p>   integer NELEMENTS<br>   parameter(NELEMENTS=6)<br>   integer numtasks, rank, source, dest, tag, i,  ierr<br>   integer blocklengths(0:1), displacements(0:1)<br>   real*4 a(0:15), b(0:NELEMENTS-1)<br>   integer <font color="#DF4442">stat(MPI_STATUS_SIZE)</font><br>   integer <font color="#DF4442">indextype</font>   <font color="#AAAAAA">! required variable</font><br>   tag = 1</p>
<p>   data a  /1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, &amp;<br>            9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0 /</p>
<p>   call <font color="#DF4442">MPI_INIT</font>(ierr)<br>   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)<br>   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)</p>
<p>   blocklengths(0) = 4<br>   blocklengths(1) = 2<br>   displacements(0) = 5<br>   displacements(1) = 12</p>
<p>   <font color="#AAAAAA">! create indexed derived data type</font><br>   call <font color="#DF4442">MPI_TYPE_INDEXED</font>(2, blocklengths, displacements, MPI_REAL, &amp;<br>                         indextype, ierr)<br>   call <font color="#DF4442">MPI_TYPE_COMMIT</font>(indextype, ierr)</p>
<p>   if (rank .eq. 0) then<br>      <font color="#AAAAAA">! task 0 sends one element of indextype to all tasks</font><br>      do i=0, numtasks-1<br>      call <font color="#DF4442">MPI_SEND</font>(a, 1, indextype, i, tag, MPI_COMM_WORLD, ierr)<br>      end do<br>   endif</p>
<p>   <font color="#AAAAAA">! all tasks receive indextype data from task 0</font><br>   source = 0<br>   call <font color="#DF4442">MPI_RECV</font>(b, NELEMENTS, MPI_REAL, source, tag, MPI_COMM_WORLD, &amp;<br>                 stat, ierr)<br>   print *, ‘rank= ‘,rank,’ b= ‘,b</p>
<p>   <font color="#AAAAAA">! free datatype when done using it</font><br>   call <font color="#DF4442">MPI_TYPE_FREE</font>(indextype, ierr)<br>   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)</p>
<p>   end</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<p> Sample program output: </p>
 <pre>rank= 0  b= 6.0 7.0 8.0 9.0 13.0 14.0
rank= 1  b= 6.0 7.0 8.0 9.0 13.0 14.0
rank= 2  b= 6.0 7.0 8.0 9.0 13.0 14.0
rank= 3  b= 6.0 7.0 8.0 9.0 13.0 14.0
</pre>

<h3 id="示例-结构派生的数据类型"><a href="#示例-结构派生的数据类型" class="headerlink" title="示例:结构派生的数据类型"></a>示例:结构派生的数据类型</h3><p>创建表示粒子的数据类型，并将这些粒子的数组分发给所有进程。</p>
<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Struct Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define NELEM 25

<p>   main(int argc, char *argv[])  {<br>   int numtasks, rank, source=0, dest, tag=1, i;</p>
<p>   typedef struct {<br>     float x, y, z;<br>     float velocity;<br>     int  n, type;<br>     }          Particle;<br>   Particle     p[NELEM], particles[NELEM];<br>   <font color="#DF4442">MPI_Datatype particletype, oldtypes[2]</font>;   <font color="#AAAAAA">// required variables</font><br>   int          blockcounts[2];</p>
<p>   <font color="#AAAAAA">// MPI_Aint type used to be consistent with syntax of</font><br>   <font color="#AAAAAA">// MPI_Type_extent routine</font><br>   <font color="#DF4442">MPI_Aint    offsets[2], extent</font>;</p>
<p>   <font color="#DF4442">MPI_Status stat</font>;</p>
<p>   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);<br>   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);<br>   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);</p>
<p>   <font color="#AAAAAA">// setup description of the 4 MPI_FLOAT fields x, y, z, velocity</font><br>   offsets[0] = 0;<br>   oldtypes[0] = MPI_FLOAT;<br>   blockcounts[0] = 4;</p>
<p>   <font color="#AAAAAA">// setup description of the 2 MPI_INT fields n, type</font><br>   <font color="#AAAAAA">// need to first figure offset by getting size of MPI_FLOAT</font><br>   <font color="#DF4442">MPI_Type_extent</font>(MPI_FLOAT, &amp;extent);<br>   offsets[1] = 4 * extent;<br>   oldtypes[1] = MPI_INT;<br>   blockcounts[1] = 2;</p>
<p>   <font color="#AAAAAA">// define structured type and commit it</font><br>   <font color="#DF4442">MPI_Type_struct</font>(2, blockcounts, offsets, oldtypes, &amp;particletype);<br>   <font color="#DF4442">MPI_Type_commit</font>(&amp;particletype);</p>
<p>   <font color="#AAAAAA">// task 0 initializes the particle array and then sends it to each task</font><br>   if (rank == 0) {<br>     for (i=0; i&lt;NELEM; i++) {<br>        particles[i].x = i * 1.0;<br>        particles[i].y = i * -1.0;<br>        particles[i].z = i * 1.0;<br>        particles[i].velocity = 0.25;<br>        particles[i].n = i;<br>        particles[i].type = i % 2;<br>        }<br>     for (i=0; i&lt;numtasks; i++)<br>        <font color="#DF4442">MPI_Send</font>(particles, NELEM, particletype, i, tag, MPI_COMM_WORLD);<br>     }</p>
<p>   <font color="#AAAAAA">// all tasks receive particletype data</font><br>   <font color="#DF4442">MPI_Recv</font>(p, NELEM, particletype, source, tag, MPI_COMM_WORLD, &amp;stat);</p>
<p>   printf(“rank= %d   %3.2f %3.2f %3.2f %3.2f %d %d\n”, rank,p[3].x,<br>        p[3].y,p[3].z,p[3].velocity,p[3].n,p[3].type);</p>
<p>   <font color="#AAAAAA">// free datatype when done using it</font><br>   <font color="#DF4442">MPI_Type_free</font>(&amp;particletype);<br>   <font color="#DF4442">MPI_Finalize</font>();<br>   }</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortan - Struct Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program struct
   include <font color="#DF4442">'mpif.h'</font>

<p>   integer NELEM<br>   parameter(NELEM=25)<br>   integer numtasks, rank, source, dest, tag, i,  ierr<br>   integer <font color="#DF4442">stat(MPI_STATUS_SIZE)</font></p>
<p>   type Particle<br>   sequence<br>   real*4 x, y, z, velocity<br>   integer n, type<br>   end type Particle</p>
<p>   type (Particle) p(NELEM), particles(NELEM)<br>   integer <font color="#DF4442">particletype, oldtypes(0:1)</font>   <font color="#AAAAAA">! required variables</font><br>   integer blockcounts(0:1), offsets(0:1), extent<br>   tag = 1</p>
<p>   call <font color="#DF4442">MPI_INIT</font>(ierr)<br>   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)<br>   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)</p>
<p>   <font color="#AAAAAA">! setup description of the 4 MPI_REAL fields x, y, z, velocity</font><br>   offsets(0) = 0<br>   oldtypes(0) = MPI_REAL<br>   blockcounts(0) = 4</p>
<p>   <font color="#AAAAAA">! setup description of the 2 MPI_INTEGER fields n, type</font><br>   <font color="#AAAAAA">! need to first figure offset by getting size of MPI_REAL</font><br>   call <font color="#DF4442">MPI_TYPE_EXTENT</font>(MPI_REAL, extent, ierr)<br>   offsets(1) = 4 * extent<br>   oldtypes(1) = MPI_INTEGER<br>   blockcounts(1) = 2</p>
<p>   <font color="#AAAAAA">! define structured type and commit it</font><br>   call <font color="#DF4442">MPI_TYPE_STRUCT</font>(2, blockcounts, offsets, oldtypes, &amp;<br>                        particletype, ierr)<br>   call <font color="#DF4442">MPI_TYPE_COMMIT</font>(particletype, ierr)</p>
<p>   <font color="#AAAAAA">! task 0 initializes the particle array and then sends it to each task</font><br>   if (rank .eq. 0) then<br>      do i=0, NELEM-1<br>      particles(i) = Particle ( 1.0<em>i, -1.0</em>i, 1.0*i, 0.25, i, mod(i,2) )<br>      end do</p>
<pre><code>do i=0, numtasks-1
call &lt;font color=&quot;#DF4442&quot;&gt;MPI_SEND&lt;/font&gt;(particles, NELEM, particletype, i, tag, &amp;amp;
              MPI_COMM_WORLD, ierr)
end do</code></pre><p>   endif</p>
<p>   <font color="#AAAAAA">! all tasks receive particletype data</font><br>   source = 0<br>   call <font color="#DF4442">MPI_RECV</font>(p, NELEM, particletype, source, tag, &amp;<br>                 MPI_COMM_WORLD, stat, ierr)</p>
<p>   print *, ‘rank= ‘,rank,’ p(3)= ‘,p(3)</p>
<p>   <font color="#AAAAAA">! free datatype when done using it</font><br>   call <font color="#DF4442">MPI_TYPE_FREE</font>(particletype, ierr)<br>   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)<br>   end</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<p> Sample program output: </p>
 <pre>rank= 0   3.00 -3.00 3.00 0.25 3 1
rank= 2   3.00 -3.00 3.00 0.25 3 1
rank= 1   3.00 -3.00 3.00 0.25 3 1
rank= 3   3.00 -3.00 3.00 0.25 3 1
</pre>

<h2 id="组和通信子管理例程"><a href="#组和通信子管理例程" class="headerlink" title="组和通信子管理例程"></a>组和通信子管理例程</h2><h3 id="Groups-vs-Communicators"><a href="#Groups-vs-Communicators" class="headerlink" title="Groups vs. Communicators:"></a>Groups vs. Communicators:</h3><ul>
<li>组是一组有序的进程集合。组中的每个进程都与一个惟一的整数rank相关联。rank值从0到N-1，其中N是组中的进程数。在MPI中，组在系统内存中表示为一个对象。程序员只能通过一个“句柄”来访问它。组总是与通信器对象关联。</li>
<li>通信器包含一组可以相互通信的进程。所有MPI消息必须指定一个通信器。从最简单的意义上说，通信器是一个额外的“标记”，必须包含在MPI调用中。像组一样，通信器在系统内存中表示为对象，程序员只能通过“句柄”访问它。例如，包含所有任务的通信器的句柄是MPI_COMM_WORLD。</li>
<li>从程序员的角度来看，一个组和一个通信者是一体的。组例程主要用于指定应该使用哪些进程来构造通信器。</li>
</ul>
<h3 id="组和通信器对象的主要用途"><a href="#组和通信器对象的主要用途" class="headerlink" title="组和通信器对象的主要用途"></a>组和通信器对象的主要用途</h3><ol>
<li>允许您根据功能将任务组织为任务组。</li>
<li>启用跨相关任务子集的集体通信操作。</li>
<li>为实现用户定义的虚拟拓扑提供基础</li>
<li>提供安全通讯</li>
</ol>
<h3 id="编程注意事项和限制-1"><a href="#编程注意事项和限制-1" class="headerlink" title="编程注意事项和限制"></a>编程注意事项和限制</h3><ul>
<li>组/通信子是动态的——它可以在程序执行期间创建和销毁</li>
<li>过程可以是在一个以上的组/通信子。他们在各组/通信子的rank是唯一的。</li>
<li>MPI提供了40多个与组、通信器和虚拟拓扑相关的例程。</li>
<li>Typical usage: <ol>
<li>使用MPI_COMM_group从MPI_COMM_WORLD中提取全局group的句柄</li>
<li>使用MPI_group_incl形成新组作为全局组的子集</li>
<li>使用MPI_Comm_Create为新组创建新的通信器</li>
<li>使用MPI_Comm_rank确定新通信器中的新rank</li>
<li>使用任何MPI消息传递例程进行通信</li>
<li>完成后，使用MPI_Comm_free和MPI_group_free释放新的通信和组(可选)</li>
</ol>
</li>
</ul>
<h3 id="Group-and-Communicator-Management-Routines"><a href="#Group-and-Communicator-Management-Routines" class="headerlink" title="Group and Communicator Management Routines"></a>Group and Communicator Management Routines</h3><p>为单独的集体通信交换创建两个不同的进程组。还需要创建新的通信器。</p>
<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Group and Communicator Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define NPROCS 8

<p>   main(int argc, char *argv[])  {<br>   int        rank, new_rank, sendbuf, recvbuf, numtasks,<br>              ranks1[4]={0,1,2,3}, ranks2[4]={4,5,6,7};<br>   <font color="DF4442">MPI_Group  orig_group, new_group</font>;   <font color="#AAAAAA">// required variables</font><br>   <font color="DF4442">MPI_Comm   new_comm</font>;   <font color="#AAAAAA">// required variable</font></p>
<p>   <font color="DF4442">MPI_Init</font>(&amp;argc,&amp;argv);<br>   <font color="DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);<br>   <font color="DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);</p>
<p>   if (numtasks != NPROCS) {<br>     printf(“Must specify MP_PROCS= %d. Terminating.\n”,NPROCS);<br>     <font color="DF4442">MPI_Finalize</font>();<br>     exit(0);<br>     }</p>
<p>   sendbuf = rank;</p>
<p>   <font color="#AAAAAA">// extract the original group handle</font><br>   <font color="DF4442">MPI_Comm_group</font>(MPI_COMM_WORLD, &amp;orig_group);</p>
<p>   <font color="#AAAAAA">//  divide tasks into two distinct groups based upon rank</font><br>   if (rank &lt; NPROCS/2) {<br>     <font color="DF4442">MPI_Group_incl</font>(orig_group, NPROCS/2, ranks1, &amp;new_group);<br>     }<br>   else {<br>     <font color="DF4442">MPI_Group_incl</font>(orig_group, NPROCS/2, ranks2, &amp;new_group);<br>     }</p>
<p>   <font color="#AAAAAA">// create new new communicator and then perform collective communications</font><br>   <font color="DF4442">MPI_Comm_create</font>(MPI_COMM_WORLD, new_group, &amp;new_comm);<br>   <font color="DF4442">MPI_Allreduce</font>(&amp;sendbuf, &amp;recvbuf, 1, MPI_INT, MPI_SUM, new_comm);</p>
<p>   <font color="#AAAAAA">// get rank in new group</font><br>   <font color="DF4442">MPI_Group_rank</font> (new_group, &amp;new_rank);<br>   printf(“rank= %d newrank= %d recvbuf= %d\n”,rank,new_rank,recvbuf);</p>
<p>   <font color="DF4442">MPI_Finalize</font>();<br>   }</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Group and Communicator Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program group
   include <font color="DF4442">'mpif.h'</font>

<p>   integer NPROCS<br>   parameter(NPROCS=8)<br>   integer rank, new_rank, sendbuf, recvbuf, numtasks<br>   integer ranks1(4), ranks2(4), ierr<br>   integer <font color="DF4442">orig_group, new_group, new_comm</font>   <font color="#AAAAAA">! required variables</font><br>   data ranks1 /0, 1, 2, 3/, ranks2 /4, 5, 6, 7/</p>
<p>   call <font color="DF4442">MPI_INIT</font>(ierr)<br>   call <font color="DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)<br>   call <font color="DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)</p>
<p>   if (numtasks .ne. NPROCS) then<br>     print *, ‘Must specify NPROCS= ‘,NPROCS,’ Terminating.’<br>     call <font color="DF4442">MPI_FINALIZE</font>(ierr)<br>     stop<br>   endif</p>
<p>   sendbuf = rank</p>
<p>   <font color="#AAAAAA">! extract the original group handle</font><br>   call <font color="DF4442">MPI_COMM_GROUP</font>(MPI_COMM_WORLD, orig_group, ierr)</p>
<p>   <font color="#AAAAAA">! divide tasks into two distinct groups based upon rank</font><br>   if (rank .lt. NPROCS/2) then<br>      call <font color="DF4442">MPI_GROUP_INCL</font>(orig_group, NPROCS/2, ranks1, new_group, ierr)<br>   else<br>      call <font color="DF4442">MPI_GROUP_INCL</font>(orig_group, NPROCS/2, ranks2, new_group, ierr)<br>   endif</p>
<p>   <font color="#AAAAAA">! create new new communicator and then perform collective communications</font><br>   call <font color="DF4442">MPI_COMM_CREATE</font>(MPI_COMM_WORLD, new_group, new_comm, ierr)<br>   call <font color="DF4442">MPI_ALLREDUCE</font>(sendbuf, recvbuf, 1, MPI_INTEGER, MPI_SUM, new_comm, ierr)</p>
<p>   <font color="#AAAAAA">! get rank in new group</font><br>   call <font color="DF4442">MPI_GROUP_RANK</font>(new_group, new_rank, ierr)<br>   print *, ‘rank= ‘,rank,’ newrank= ‘,new_rank,’ recvbuf= ‘, recvbuf</p>
<p>   call <font color="DF4442">MPI_FINALIZE</font>(ierr)<br>   end</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<p> Sample program output: </p>
 <pre>rank= 7 newrank= 3 recvbuf= 22
rank= 0 newrank= 0 recvbuf= 6
rank= 1 newrank= 1 recvbuf= 6
rank= 2 newrank= 2 recvbuf= 6
rank= 6 newrank= 2 recvbuf= 22
rank= 3 newrank= 3 recvbuf= 6
rank= 4 newrank= 0 recvbuf= 22
rank= 5 newrank= 1 recvbuf= 22
</pre>

<h2 id="虚拟拓扑-Virtual-Topologies"><a href="#虚拟拓扑-Virtual-Topologies" class="headerlink" title="虚拟拓扑(Virtual Topologies)"></a>虚拟拓扑(Virtual Topologies)</h2><h3 id="What-Are-They"><a href="#What-Are-They" class="headerlink" title="What Are They?"></a>What Are They?</h3><ul>
<li>In terms of MPI, a virtual topology describes a mapping/ordering of MPI processes into a geometric “shape”. </li>
<li>MPI支持的两种主要拓扑类型是笛卡尔(网格)和图。</li>
<li>MPI拓扑是虚拟的——并行机的物理结构和进程拓扑之间可能没有关系。</li>
<li>虚拟拓扑构建在MPI通信器和组之上。</li>
<li>必须由应用程序开发人员“编程”。</li>
</ul>
<h3 id="Why-Use-Them"><a href="#Why-Use-Them" class="headerlink" title="Why Use Them?"></a>Why Use Them?</h3><ul>
<li>Convenience <ul>
<li>虚拟拓扑可能对具有特定通信模式(与MPI拓扑结构匹配的模式)的应用程序有用。</li>
<li>例如，对于需要对基于网格的数据进行4路最近邻通信的应用程序，笛卡尔拓扑可能很方便。</li>
</ul>
</li>
<li>Communication Efficiency <ul>
<li>一些硬件架构可能会对相继相隔很远的“节点”之间的通信进行惩罚。</li>
<li>特定的实现可以根据给定并行机的物理特性来优化进程映射。</li>
<li>进程到MPI虚拟拓扑的映射依赖于MPI实现，可能完全被忽略。</li>
</ul>
</li>
</ul>
<h3 id="虚拟拓扑的例程"><a href="#虚拟拓扑的例程" class="headerlink" title="虚拟拓扑的例程"></a>虚拟拓扑的例程</h3><p> Create a 4 x 4 Cartesian topology from 16 processors and have each process exchange its rank with four neighbors. </p>
 <table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Cartesian Virtual Topology Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define SIZE 16
   #define UP    0
   #define DOWN  1
   #define LEFT  2
   #define RIGHT 3

<p>   main(int argc, char *argv[])  {<br>   int numtasks, rank, source, dest, outbuf, i, tag=1,<br>      inbuf[4]={MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,},<br>      nbrs[4], dims[2]={4,4},<br>      periods[2]={0,0}, reorder=0, coords[2];</p>
<p>   <font color="#DF4442">MPI_Request reqs[8]</font>;<br>   <font color="#DF4442">MPI_Status stats[8]</font>;<br>   <font color="#DF4442">MPI_Comm cartcomm</font>;   <font color="#AAAAAA">// required variable</font></p>
<p>   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);<br>   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);</p>
<p>   if (numtasks == SIZE) {<br>      <font color="#AAAAAA">// create cartesian virtual topology, get rank, coordinates, neighbor ranks</font><br>      <font color="#DF4442">MPI_Cart_create</font>(MPI_COMM_WORLD, 2, dims, periods, reorder, &amp;cartcomm);<br>      <font color="#DF4442">MPI_Comm_rank</font>(cartcomm, &amp;rank);<br>      <font color="#DF4442">MPI_Cart_coords</font>(cartcomm, rank, 2, coords);<br>      <font color="#DF4442">MPI_Cart_shift</font>(cartcomm, 0, 1, &amp;nbrs[UP], &amp;nbrs[DOWN]);<br>      <font color="#DF4442">MPI_Cart_shift</font>(cartcomm, 1, 1, &amp;nbrs[LEFT], &amp;nbrs[RIGHT]);</p>
<pre><code>printf(&quot;rank= %d coords= %d %d  neighbors(u,d,l,r)= %d %d %d %d\n&quot;,
       rank,coords[0],coords[1],nbrs[UP],nbrs[DOWN],nbrs[LEFT],
       nbrs[RIGHT]);

outbuf = rank;

&lt;font color=&quot;#AAAAAA&quot;&gt;// exchange data (rank) with 4 neighbors&lt;/font&gt;
for (i=0; i&amp;lt;4; i++) {
   dest = nbrs[i];
   source = nbrs[i];
   &lt;font color=&quot;#DF4442&quot;&gt;MPI_Isend&lt;/font&gt;(&amp;amp;outbuf, 1, MPI_INT, dest, tag, 
             MPI_COMM_WORLD, &amp;amp;reqs[i]);
   &lt;font color=&quot;#DF4442&quot;&gt;MPI_Irecv&lt;/font&gt;(&amp;amp;inbuf[i], 1, MPI_INT, source, tag, 
             MPI_COMM_WORLD, &amp;amp;reqs[i+4]);
   }

&lt;font color=&quot;#DF4442&quot;&gt;MPI_Waitall&lt;/font&gt;(8, reqs, stats);

printf(&quot;rank= %d                  inbuf(u,d,l,r)= %d %d %d %d\n&quot;,
       rank,inbuf[UP],inbuf[DOWN],inbuf[LEFT],inbuf[RIGHT]);  }</code></pre><p>   else<br>      printf(“Must specify %d processors. Terminating.\n”,SIZE);</p>
<p>   <font color="#DF4442">MPI_Finalize</font>();<br>   }</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<table width="90%" cellspacing="0" cellpadding="15" border="1"><tbody><tr><td> <!---outer table--->
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tbody><tr>
<td colspan="2" width="30" bgcolor="FOF5FE" align="center"><img src="../images/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Cartesian Virtual Topology Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program cartesian
   include <font color="#DF4442">'mpif.h'</font>

<p>   integer SIZE, UP, DOWN, LEFT, RIGHT<br>   parameter(SIZE=16)<br>   parameter(UP=1)<br>   parameter(DOWN=2)<br>   parameter(LEFT=3)<br>   parameter(RIGHT=4)<br>   integer numtasks, rank, source, dest, outbuf, i, tag, ierr, &amp;<br>           inbuf(4), nbrs(4), dims(2), coords(2), periods(2), reorder<br>   integer <font color="#DF4442">stats(MPI_STATUS_SIZE, 8), reqs(8)</font><br>   integer <font color="#DF4442">cartcomm</font>   <font color="#AAAAAA">! required variable</font><br>   data inbuf /MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL/, &amp;<br>        dims /4,4/, tag /1/, periods /0,0/, reorder /0/ </p>
<p>   call <font color="#DF4442">MPI_INIT</font>(ierr)<br>   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)</p>
<p>   if (numtasks .eq. SIZE) then<br>      <font color="#AAAAAA">! create cartesian virtual topology, get rank, coordinates, neighbor ranks</font><br>      call <font color="#DF4442">MPI_CART_CREATE</font>(MPI_COMM_WORLD, 2, dims, periods, reorder, &amp;<br>                           cartcomm, ierr)<br>      call <font color="#DF4442">MPI_COMM_RANK</font>(cartcomm, rank, ierr)<br>      call <font color="#DF4442">MPI_CART_COORDS</font>(cartcomm, rank, 2, coords, ierr)<br>      call <font color="#DF4442">MPI_CART_SHIFT</font>(cartcomm, 0, 1, nbrs(UP), nbrs(DOWN), ierr)<br>      call <font color="#DF4442">MPI_CART_SHIFT</font>(cartcomm, 1, 1, nbrs(LEFT), nbrs(RIGHT), ierr)</p>
<pre><code>write(*,20) rank,coords(1),coords(2),nbrs(UP),nbrs(DOWN), &amp;amp;
            nbrs(LEFT),nbrs(RIGHT)

&lt;font color=&quot;#AAAAAA&quot;&gt;! exchange data (rank) with 4 neighbors&lt;/font&gt;
outbuf = rank
do i=1,4
   dest = nbrs(i)
   source = nbrs(i)
   call &lt;font color=&quot;#DF4442&quot;&gt;MPI_ISEND&lt;/font&gt;(outbuf, 1, MPI_INTEGER, dest, tag, &amp;amp;
                 MPI_COMM_WORLD, reqs(i), ierr)
   call &lt;font color=&quot;#DF4442&quot;&gt;MPI_IRECV&lt;/font&gt;(inbuf(i), 1, MPI_INTEGER, source, tag, &amp;amp;
                 MPI_COMM_WORLD, reqs(i+4), ierr)
enddo

call &lt;font color=&quot;#DF4442&quot;&gt;MPI_WAITALL&lt;/font&gt;(8, reqs, stats, ierr)

write(*,30) rank,inbuf</code></pre><p>   else<br>     print *, ‘Must specify’,SIZE,’ processors.  Terminating.’<br>   endif</p>
<p>   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)</p>
<p>   20 format(‘rank= ‘,I3,’ coords= ‘,I2,I2, &amp;<br>             ‘ neighbors(u,d,l,r)= ‘,I3,I3,I3,I3 )<br>   30 format(‘rank= ‘,I3,’                 ‘, &amp;<br>             ‘ inbuf(u,d,l,r)= ‘,I3,I3,I3,I3 )</p>
<p>   end</p>
<p></pre></td></p>
</tr></tbody></table>
</td></tr></tbody></table>

<p> Sample program output: (partial) </p>
 <pre>rank=   0 coords=  0 0 neighbors(u,d,l,r)=  -1  4 -1  1
rank=   0                  inbuf(u,d,l,r)=  -1  4 -1  1
rank=   8 coords=  2 0 neighbors(u,d,l,r)=   4 12 -1  9
rank=   8                  inbuf(u,d,l,r)=   4 12 -1  9
rank=   1 coords=  0 1 neighbors(u,d,l,r)=  -1  5  0  2
rank=   1                  inbuf(u,d,l,r)=  -1  5  0  2
rank=  13 coords=  3 1 neighbors(u,d,l,r)=   9 -1 12 14
rank=  13                  inbuf(u,d,l,r)=   9 -1 12 14
...
...
rank=   3 coords=  0 3 neighbors(u,d,l,r)=  -1  7  2 -1
rank=   3                  inbuf(u,d,l,r)=  -1  7  2 -1
rank=  11 coords=  2 3 neighbors(u,d,l,r)=   7 15 10 -1
rank=  11                  inbuf(u,d,l,r)=   7 15 10 -1
rank=  10 coords=  2 2 neighbors(u,d,l,r)=   6 14  9 11
rank=  10                  inbuf(u,d,l,r)=   6 14  9 11
rank=   9 coords=  2 1 neighbors(u,d,l,r)=   5 13  8 10
rank=   9                  inbuf(u,d,l,r)=   5 13  8 10
</pre>

<h2 id="简要介绍一下MPI-2和MPI-3"><a href="#简要介绍一下MPI-2和MPI-3" class="headerlink" title="简要介绍一下MPI-2和MPI-3"></a>简要介绍一下MPI-2和MPI-3</h2><h3 id="MPI-2"><a href="#MPI-2" class="headerlink" title="MPI-2:"></a>MPI-2:</h3><ul>
<li>有意地，MPI-1规范没有解决几个“困难的”问题。出于权宜之计，这些问题在1998年被推迟到第二个规范，称为MPI-2。</li>
<li>MPI-2是对MPI-1的重大修订，增加了新的功能和修正。</li>
<li>MPI-2新功能的关键领域<ul>
<li><strong>动态进程</strong>——删除MPI静态进程模型的扩展。提供在作业启动后创建新进程的例程。</li>
<li><strong>One-Sided Communications</strong> - provides routines for one directional communications. Include shared memory operations (put/get) and remote accumulate operations. </li>
<li><strong>扩展的集合操作</strong>-允许应用集合操作的内部通信</li>
<li><strong>外部接口</strong>——定义允许开发人员在MPI之上构建的例程，比如调试器和分析器。</li>
<li><strong>其他语言绑定</strong>——描述c++绑定并讨论Fortran-90问题。</li>
<li><strong>Parallel I/O</strong> - describes MPI support for parallel I/O. </li>
</ul>
</li>
</ul>
<h3 id="MPI-3"><a href="#MPI-3" class="headerlink" title="MPI-3:"></a>MPI-3:</h3><ul>
<li>MPI-3标准于2012年采用，包含了对MPI-1和MPI-2功能的重要扩展，包括<ul>
<li><strong>非阻塞的集合操作</strong>——允许集合中的任务在没有阻塞的情况下执行操作，可能提供性能改进。</li>
<li><strong>新的单边通信操作</strong>-更好地处理不同的内存模型。</li>
<li><strong>邻域集合</strong>——扩展了分布式图和笛卡尔进程拓扑，增加了通信能力。</li>
<li><strong>Fortran</strong> 2008 Bindings - expanded from Fortran90 bindings </li>
<li><strong>MPIT Tool Interface</strong> - allows the MPI implementation to expose certain internal variables, counters, and other states to the user (most likely performance tools). </li>
<li><strong>Matched Probe</strong> - fixes an old bug in MPI-2 where one could not probe for messages in a multi-threaded environment. </li>
</ul>
</li>
</ul>
<h3 id="More-Information-on-MPI-2-and-MPI-3"><a href="#More-Information-on-MPI-2-and-MPI-3" class="headerlink" title="More Information on MPI-2 and MPI-3:"></a>More Information on MPI-2 and MPI-3:</h3><ul>
<li>  MPI Standard documents: <a href="http://www.mpi-forum.org/docs/" target="_blank" rel="noopener">http://www.mpi-forum.org/docs/</a> </li>
</ul>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 jaytp@qq.com </span>
    </div>
</article>


<p>
    <a  class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>MPITutorials</p>
    
    <p><span class="copy-title">本文作者:</span><a  title="Linuzb">Linuzb</a></p>
    <p><span class="copy-title">发布时间:</span>2020-05-26, 17:42:38</p>
    <p><span class="copy-title">最后更新:</span>2020-06-08, 15:45:33</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/2020/05/26/MPITutorials/" title="MPITutorials">https://levizebulon.github.io/2020/05/26/MPITutorials/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>





    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2016-2020 Linuzb</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class="" id="rocket" ></a>

    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close"  onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div>


</body>
<script src="/js/jquery.pjax.js?v=1.0.1" ></script>

<script src="/js/script.js?v=1.0.1" ></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['#Machine Learning','#工具','#阅读','#Linux','#paper','#Distribute System','#Web','#software','#Others','#Algarithm','#问题','#论文','#杂项','#course','#tools','#distribute system','#os','#HPC','#programming','#Deep Learning','#Computer Graphics',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().trim().split('\n').length, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: #fafafa;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 492px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    
    .post .pjax article .article-entry>ol, .post .pjax article .article-entry>ul, .post .pjax article>ol, .post .pjax article>ul{
        border: #e2dede solid 1px;
        border-radius: 10px;
        padding: 10px 32px 10px 56px;
    }
    .post .pjax article .article-entry li>ol, .post .pjax article .article-entry li>ul,.post .pjax article li>ol, .post .pjax article li>ul{
        padding-top: 5px;
        padding-bottom: 5px;
    }
    .post .pjax article .article-entry>ol>li, .post .pjax article .article-entry>ul>li,.post .pjax article>ol>li, .post .pjax article>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    .post .pjax article .article-entry li>ol>li, .post .pjax article .article-entry li>ul>li,.post .pjax article li>ol>li, .post .pjax article li>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    

    
</style>







</html>
